\pdfoutput=1
\documentclass{article}
\usepackage[sort,authoryear]{natbib}
\usepackage{mathtools}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{enumitem}
\usepackage[margin=1in]{geometry}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{xcolor}
\usepackage[colorlinks = true, citecolor = blue, urlcolor = blue]{hyperref}
\usepackage{indentfirst}
\usepackage{apptools}
\usepackage{cleveref}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}

\graphicspath{{fig/}}

\AtAppendix{\counterwithin{lemma}{section}}
\AtAppendix{\counterwithin{proposition}{section}}

\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}

\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\argmax}{argmax}

\newtheorem{lemma}{Lemma} 
\newtheorem{example}{Example} 
\newtheorem{definition}{Definition} 
\newtheorem{proposition}{Proposition}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}

\newcommand{\Cov}{\textup{Cov}}
\newcommand{\Var}{\textup{Var}}
\newcommand{\R}{\mathbb{R}}

\providecommand{\keywords}[1]
{
  \textbf{\textit{Keywords---}} #1
}




\title{Selective inference is easier with p-values}
\author{Anav Sood\\ Stanford University}
\date{\today}

\begin{document}

\maketitle

\begin{abstract}
Selective inference is a subfield of statistics that enables valid inferences after selection of a data-dependent question. In this paper, we introduce selectively dominant p-values, a class of p-values that allow practitioners to easily perform selective inference after arbitrary selection procedures. Unlike a traditional p-value, whose distribution must stochastically dominate the uniform distribution under the null, a selectively dominant p-value must have a  post-selection distribution that stochastically dominates that of a uniform having undergone the same selection process; moreover, this property must hold simultaneously for all possible selection processes. Despite the strength of this condition, we show that all commonly used p-values (e.g., p-values from two-sided testing in parametric families, one-sided testing in monotone likelihood ratio and exponential families, and permutation tests) are selectively dominant. By recasting two canonical selective inference problems—inference on winners and rank verification—in our selective dominance framework, we provide simpler derivations, a deeper conceptual understanding, and new generalizations and variations of these methods. Additionally, we use our insights to introduce selective variants of methods that combine p-values, such as Fisher's combination test. 
\end{abstract}


\section{Introduction}

Selective inference is a subfield of statistics that allows practioners to make valid inferences even when statistical question at hand was chosen by a data-driven selection process. Many selective methods however, which operate by conditioning on the selection event, can be difficult to derive, hard to implement, and exhibit counterintuitive behaviors. To statisticians outside of the field, each selective procedure may seem to come from a different argument or approach.

In this paper, we provide a unifying framework for selective inference centered around p-values. So long as a statistician knows how to construct a p-value for their inferential question at hand, our framework provides an algorithmic approach for delivering hypothesis testing procedures and confidence intervals that are valid even after selection. Our framework (1) can greatly simplify the process of designing new selective methods and (2) results in more natural and general derivations of some existing selective methods, allowing for a deeper understanding of their behavior as well as new variations and extensions. 


\subsection{Motivation}

\begin{figure}
    \centering
    \scalebox{1}{
    \hspace{-0.025 \textwidth}
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{fig/class_dist_to_winner.pdf}
    \end{minipage}
    \hspace{0.05 \textwidth}
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{fig/cond_dist_to_winner.pdf}
    \end{minipage}
    }
    \caption{ The first panel (left) shows the growth of the quantile $z_{1 - \alpha_n}$ as a function of the dimension $n$. The second panel (right) gives the distance between the level $\alpha=0.05$ conditional LCB and the winner $X_W$ as a function of the gap $X_W - X_R$ between winner and the runner-up. For dimensions $n=1,10,100,1000$, it also gives the distance from $X_W$ to the level $\alpha=0.05$ simultaneous LCB.}
    \label{fig:winner}
\end{figure}

As motivation, we consider the setting of independent Gaussian data $X \sim N(\mu, I_n)$ with unknown mean $\mu$ and recall the problem of doing inference on the winner. Since the largest observation $W = \argmax_{i \in [n]} X_i$ is likely to correspond to the largest mean, a natural way to verify the existence of a large effect (i.e., a large $\mu_i$) is to give a lower confidence bound (LCB) for $\mu_W$, the mean of the winning value. 

Normally we provide such an LCB via Sidak's simultaneous approach. Letting $z_{1-\alpha}$ denote the $1-alpha$ quantile of the standard normal distribution and defining $\alpha_n = 1 - (1-\alpha)^\frac{1}{n}$, the lower bounds $ \mu_i > X_i - z_{1 - \alpha_n}$ hold simultaneously with probability $1-\alpha$. Therefore, $\hat{\mu}_{simul} = X_W - z_{1 - \alpha_n}$ is a valid lower bound for $\mu_W$ that holds with probability at least $1-\alpha$. Performing simultaneous inference on $n$ means, however, comes at a cost. As $n$ grows, the quantile $z_{1 - \alpha_n}$ grows as well (depicted in the left panel of \Cref{fig:winner}), and the distance from the winning observation to the LCB correspondingly increases. 

A more modern approach is to provide a LCB that is valid conditionally on $W$. This approach provides inferences for only the winning mean $\mu_W$ and no other means, so we may hope that it avoids the simultaneous inference's curse of dimensionality. Following the recipe of \cite{Fithian2017}, one finds the conditional LCB to be 
\begin{equation}
    \label{eq:motivating_conditional_lcb}
    \hat{\mu}_{cond} = \inf \{\mu_0 : \mu_0 > X_W - \text{Quantile}(1-\alpha, TN(0, 1, X_R - \mu_0, \infty))  \},
\end{equation}
where $X_R$ is the runner-up (second largest) observation and $TN(\mu, \sigma^2, a, b)$ is a $N(\mu, \sigma^2)$ distribution truncated to lie in the interval $[a, b]$. 

The conditional LCB \eqref{eq:motivating_conditional_lcb} is near impossible to parse, but it turns out to have some very interesting behavior. As plotted in right panel of \Cref{fig:winner}, the distance $X_W - \hat{\mu}_{cond}$ between the winner and the conditional LCB is purely a function of the gap $X_W - X_R$ between the winner and runner-up. If the gap between $X_W$ and $X_R$ is large, then the conditional LCB for $\mu_W$ will be roughly $X_{W} - z_{1-\alpha}$, i.e, what we expect in a one-dimensional inference problem. But as the runner-up gets close to the winner, the conditional LCB explodes quickly to $-\infty$, and can give much worse inferences than the classical approach. In summary, the conditional method appears to avoid the curse of dimensionality in some situations, but when it fails to, the consequences can be tremendous. 

The motivation for this article comes from the following fact: the conditional LCB becomes shockingly simple to parse once written in terms of p-values. Imagine using each LCB to verify the existence of a positive mean, which we can do by ensuring that the LCB is non-negative. Normally, we verify the existence of positive means by testing the nulls $H_{0, i} : \mu_i \leq 0$ with the p-values $p_i = 1 - \Phi(X_i)$. It turns out that the simultaneous LCB verifies the existence of a positive mean when smallest p-value $p_{(1)}$ is at most $\alpha_n$. In contrast, the conditional LCB does so when the ratio of the top two smallest p-values $p_{(1)}/p_{(2)}$ is at most $\alpha$. 


At least when the p-values $p_i$ are uniform under the null, the smallest p-value $p_{(1)}$ should be uniform on $[0, p_{(2)}]$, and we should control error when we reject when $p_{(1)} \leq \alpha p_{(2)}$. But does this work when the p-values are not exactly uniform? Why does it work in the Gaussian case?

The framework we develop in this article allows us to easily prove the validity of the above procedure, where we reject $H_{0, (1)}$ when $p_{(1)} \leq \alpha p_{(2)}$. Moreover, t



\subsection{Our Contributions}

In this paper we introduce the \textbf{selective dominance} framework, which we summarize here. In the framework, we imagine using a p-value $p$ to test the null hypothesis $H_0$. 


The sufficient statisics can vary quite heavily from problem to problem, making many selective inference methods appear quite different. By focusing on the p-value... 

The remainder of the paper illustrates our framework's utility via a number of applications. 

%Our work has a number of novel contributions. \newline 

%\noindent \textbf{Application one - inference on winners: }\newline 

%\noindent \textbf{Application two - rank verification: }\newline 

%\noindent \textbf{Application three - new methods involving multiple p-values: } \newline 

\subsection{Related Work}

% \cite{Benjamini} finds some conditions where the inversion actually results in a CI. Also they seem to want to test a one-sided null but don't. 

O%ur goal is to provide a unified view of modern selective inference, a field started by the seminal work \cite{Lee}, that has been the subject of much research over the last decade \cite{Panigrahi,Benjamini, Tian2018Apr, Tian2018Dec, Taylor, Tibshirani, Fithian2015, Loftus ,Chen2023Apr,Chen2023May, Gao, Sengupta}. We explicitly a few concrete connections to our work. In our discussion of selecting the winner, we show that closing the resulting testing procedures results in selective sequentially valid procedures like those in \cite{Fithian2015}. A key difference is that our procedures are valid for composite null hypotheses while those in \cite{Fithian2015} are only valid for point nulls. 

%Despite this abudance of prior work, however, none of prior work presents methods that deliver valid selective tests for one-sided hypotheses like our framework allows for \footnote{\cite{Hung2019} is an exception.}. Other works provide selective methods that avoid conditioning on the selection event \cite{Zrnic, }, but \cite{W} suggest empirically that such approaches are less powerful than their conditional counterparts. 



Related work (Andrews and Fithian on selection). (Adapt for increasing p-values). Other works on selecting the winner (Tijana, Benjamini) 

Sequential selective hypothesis testing. 


%\subsection{Notation}

%Stochastic dominance.  when we say uniform we mean Unif([0, 1]). $v_{-i}$ for a vector


\section{Selectively Dominant p-Values}
\label{sec:dominance}

In this section we define selectively dominant p-values, a class of p-values that enable us to easily do inference after selection. We give a precise characterization of when p-values are selectively dominant and illustrate that the most commonly used p-values are all selectively dominant. Finally, we provide examples of how to apply our selective dominance framework. 

\subsection{Selective Dominance}

\begin{figure}[]
    \centering
    \scalebox{1}{
    \hspace{-0.03\textwidth}
    \begin{minipage}{0.33\textwidth}
        \centering
        \includegraphics[width=\textwidth]{p_dist.pdf}
    \end{minipage}\hfill
    \hspace{0.02\textwidth}
    \begin{minipage}{0.33\textwidth}
        \centering
        \includegraphics[width=\textwidth]{p_dist_given.pdf}
    \end{minipage}\hfill
    \hspace{0.02\textwidth}
    \begin{minipage}{0.33\textwidth}
        \centering
        \includegraphics[width=\textwidth]{p_sel_dist_given.pdf}
    \end{minipage}
    }
    \caption{The first panel (left) depicts an example distribution of a p-value $p$ under the null. The distribution is stochastically dominanted by the uniform distribution. The second panel (middle) depicts the distribution $p | S = 1$ of the same null p-value $p$ given that it was selected for being most $\alpha=0.1$. This distribution is \underline{not} stochastically dominated by the uniform distribution. The third panel depicts the null distribution $p_{sel}|S=1$ of \Cref{thm:adjustment}'s selective p-value $p/\alpha$ given selection. Thanks to \Cref{thm:adjustment}'s correction, this distribution again is stochastically dominanted by the uniform distribution.}
    \label{fig:distributions}
\end{figure}

In classical statistics, a p-value is a random variable $p$ supported on $[0, 1]$ that stochastically dominates the uniform distribution $U \sim \text{Unif([0, 1])}$ under the null, i.e., $p \succeq_{H_0} U$. The left-most panel of \Cref{fig:distributions} gives an example of what a null p-value distribution may look like. When working with p-values, we maintain Type I error control if we reject the null $H_0$ when $p$ is small:
\begin{equation*}
    P_{H_0}(\text{reject null}) = P_{H_0}(p \leq \alpha) \leq P(U \leq \alpha)  = \alpha.
\end{equation*}
In essence, stochastic dominance allows us to use the uniform distribution as a reference distribution. We control the probability of $p$ being small under the null by comparing it to the probability of a uniform being small. 

In the problems we consider, we use $p$ to test the null $H_0$ only after it has been ``selected''. In full generality, we consider a p-value $p$ for testing the null $H_0$ that is conditionally valid given some random vector $Z$, i.e., $p \mid Z = z \succeq_{H_0} U$:
\begin{equation}
    \label{eq:valid_given_z}
    P_{H_0}(p \leq \alpha \mid Z =z) \leq P(U \leq \alpha)  = \alpha.
\end{equation}
Additionally, we consider a binary selection random variable $S \in \{0, 1\}$ that takes value one when $p$ is selected and zero otherwise. The relationship between $p$, $Z$, and $S$ is governed by a \textbf{selection function},
\begin{equation*}
    s(x, z) = P(S = 1 \mid p = x, Z = z).
\end{equation*}
Intuitively, we imagine observing $p=x$ and $Z=z$ and then flipping a biased coin that comes up heads with probability $s(x, z)$. We only use $p$ to test the null $H_0$ when this coin comes up heads, and otherwise do not perform inference. This process turns out to capture what happens in a wide range of selective inference problems. We decide the selection procedure in the problems we consider, so $s(x, z)$ is known. In cases where no $Z$ is present, we can imagine $Z=0$ and write the selection function $s(x)$ purely in terms of $x$. 

Because we only perform inference when the coin comes up heads, our goal should be to design a procedure that controls Type I error conditional on this selection event:
\begin{equation}
    \label{eq:selective_error_control}
    P_{H_0}(\text{reject } H_0 \mid S = 1) \leq \alpha 
\end{equation}
As illustrated by our next example, the classical approach of rejecting when $p \leq \alpha$ does not maintain selective Type I error control as in \eqref{eq:selective_error_control}. 

\begin{example}[Publication bias and the failure of classical inference]
\label{exm:publication_bias}
Consider a p-value $p$ that is uniform on $[0, 1]$ under the null $H_0$. If we use the selection function $s(x) = I_{p \leq \alpha}$, i.e., we select $p$ when it is at most $\alpha$, then $p \mid S=1 \sim \text{Unif}([0, \alpha])$. Our classical procedure will clearly fail to control Type I error conditional on selection:
\begin{equation*}
    P_{H_0}(\text{reject } H_0 \mid S=1) = P_{H_0}(p \leq \alpha \mid S=1) = 1 > \alpha. 
\end{equation*}
\end{example}

\Cref{exm:publication_bias} is a standard example in the literature on publication bias. If researchers publish studies testing the null $H_0$ only when their p-values are below $\alpha$, the reader only gets to observe inferences made after this selection. As a consequence, the reader's observed type I error rate can be as high as one. \Cref{fig:distributions}'s middle panel displays the distribution of our earlier example p-value (left panel), but after it has been selected via \Cref{exm:publication_bias}'s selection function. It is clear from the picture that, after selection, the null p-value distribution is no longer stochastically dominanted by the uniform distribution. 

Essentially, after selection, the uniform distribution no longer suffices as a reference distribution. Naturally, we may instead try and use the distribution of a uniform after it has been selected by the same selection function. Formally, suppose that $U$ has uniform distribution conditional on $Z$, i..e., $U \mid Z=z \sim \text{Unif}([0, 1])$, and let $S' \in \{0, 1\}$ be a different binary selection random variable whose joint distribution with $U$ and $Z$ is governed by the same selection function 
\begin{equation*}
     P(S' = 1 \mid U = x, Z=z ) = s(x, z).
\end{equation*}
Then, we could use the conditional distribution $U \mid Z,  S' = 1$ of $U$ given selection as our reference distribution. This approach is valid exactly when our p-value is selectively dominant, as we define below \footnote{For the sake of simplicity, we require selective dominance to hold point-wise for \emph{every} $z$ rather than almost all, and our later results similarly hold point-wise. Our definition and results, however, can be modified to accomodate more general case.}. 

\begin{definition}[Selective dominance]
    \label{def:selective_dominance}
    Considering a p-value $p$ for the null $H_0$ that is valid given $Z$ as in \eqref{eq:valid_given_z}, we say that $p$ is \textbf{selectively dominant given $Z$} if, under the null $H_0$, it has a conditional probability density function (PDF) given $Z=z$ and satisfies
    \begin{equation}
    \label{eq:selective_dominance}
    p \mid Z=z, S = 1 \succeq_{H_0} U \mid Z=z, S' = 1
    \end{equation}
    for every selection function $s(x, z)$ under which $p$ and $U$ both have a positive probability of being selected given $Z=z$. 
\end{definition}

As we will soon see, the majority of p-values that practitioners use are selectively dominant as described in \eqref{eq:selective_dominance}. In \Cref{def:selective_dominance}, we restrict to p-values with conditional PDFs under the null because it makes our theory and methods simpler to state. Because we can always make a p-value both have a conditional PDF and be more powerful via randomization, this restriction is never a practical issue. Also, after applying our machinery with randomized p-values, the user can always derandomize the resulting method if they would like. %We only consider selection functions $s(x, z)$ under which $p$ and $U$ both have a positive probability of selection in \Cref{def:selective_dominance} so that the conditioning events $S = 1$ is never probability zero. 

To perform valid post-selection inference using a selectively dominant p-value, we can transform it so that it remains a p-value after selection. As \Cref{thm:adjustment} explains, we can ``undo'' the effects of selection by applying the conditional cumulative distribution function (CDF) $F_{U \mid Z, S' = 1}(\cdot)$ of $U$ given selection to $p$. In line with prior literature, we refer to this transformed p-value as a \textbf{selective p-value}. For simple selection functions, this selective p-value is often computable in closed form. 

\begin{theorem}
    \label{thm:adjustment}
    Let  $F_{U \mid Z, S '= 1}(u)$ denote the CDF of $U$ conditional on selection. Then, under the null, the selective p-value 
    \begin{equation}
    \label{eq:adjustment}
        p_{sel} = F_{U \mid Z, S' = 1}(p) = \frac{\int_0^p s(x, Z) dx}{\int_0^1 s(x, Z) dx}
    \end{equation}
    stochastically dominates the uniform distribution conditional on $Z$ and selection,
    \begin{equation}
        \label{eq:adjusted_error_control}
        P_{H_0}(p_{sel} \leq \alpha \mid Z=z, S= 1) \leq \alpha, 
    \end{equation}
    for any selection function $s(x, z)$ under which $p$ and $U$ both have a positive probability of being selected given $Z=z$. Further, for any distribution in $H_0$ under which $p$ has an exact uniform distribution given $Z=z$, \eqref{eq:adjusted_error_control} holds with equality. 
\end{theorem}

Essentially, \Cref{thm:adjustment} tells us that if we want selective Type I error control as in \eqref{eq:selective_error_control}, then we should reject $H_0$ when $p$ is less than the $\alpha$ quantile of $U \mid Z=z, S' = 1$ rather than the $\alpha$ quantile of $U$. 

The right-most panel of \Cref{fig:distributions} depicts what happens when we apply \Cref{thm:adjustment}'s correction to our example null p-value. Unlike the null distribution of $p$ given selection (middle panel), the null distribution of $p_{sel}$ (which we derive explicitly later in \Cref{exm:correction}) given selection is stochastically dominated by the uniform distribution. 

\subsection{Characterizing Selectively Dominant p-Values and Examples}

\Cref{thm:density} tells us that p-values are selectively dominant precisely when their conditional PDF is non-decreasing under the null. 

\begin{theorem}[Selective dominance and increasing density]
    \label{thm:density}
    If the conditional PDF of the p-value $p$ given $Z=z$ is always non-decreasing under the null, then it is selectively dominant given $Z$ as described in \Cref{def:selective_dominance}. Conversely, if ever under the null, the conditional PDF of $p$ given $Z=z$ is everywhere continuous and not non-decreasing, then $p$ is not selectively dominant given $Z$.  
\end{theorem}

In what follows, we give a number of examples of selectively dominant p-values. Our examples include all the common p-values that practitioners use in real life. We recommend that the unfamiliar reader review uniformly most powerful (UMP) and uniformly most powerful unbiased (UMPU) testing \cite[Chapter 3 and Chapter 4]{Lehmann} prior to proceeding.

\begin{example}[Two-sided testing in parametric families]
\label{exm:two-sided}
Consider observing data from a parametric family $P_{\theta}$ and testing the null $H_0 : \theta = \theta_0$. Because the null is a point null, most p-values we construct will have an exact $\text{Unif}([0, 1])$ distribution under the null and are therefore trivially selectively dominant. 
\end{example}

\begin{example}[One-sided testing in monotone likelihood ratio families]
\label{exm:mlr}
Consider observing one-dimensional data from a parametric family $X \sim P_{\theta}$ that admits density $p_{\theta}(x)$ with respect to some carrier measure $\mu$. We say that $P_{\theta}$ has a monotone likeliihood ratio (MLR) in the real valued function $T(x)$ if, the densities $p_{\theta}(x)$ share a common support and, for any $\theta < \theta'$, the ratio $p_{\theta'}(x)/p_{\theta}(x)$ is a non-decreasing function of $T(x)$. In this case, the UMP test for the null $H_0: \theta \leq \theta_0$ rejects when $T(X)$ is large. The associated randomized p-value for this test (see \Cref{sec:one_sided_appdx}) is selectively dominant. 
\end{example}

\begin{example}[Testing in in exponential families]
\label{exm:exp_fam}
Suppose we observe data $X \in \R^m$ from an exponential family $P_{\theta}$ parameterized by $\theta \in \R^n$ i.e., under $P_{\theta}$ the data $X$ has density  
\begin{equation*}
    g_{\theta}(x) = \exp( \theta_1 T_1(X) + \dots + \theta_n T_n(X) - \psi(\theta) ) g(x) 
\end{equation*}
with respect to some carrier measure $\mu$. In both the case of testing the two-sided null $H_0: \theta_1 \neq \theta_{0, i}$ or one-sided null $H_0: \theta_i \leq \theta_{0, i}$, the UMPU test conditions on the nuisance statistics $T_{-i}(X)$. The p-value associated with the UMPU test for $H_0: \theta_1 \neq \theta_{0, i}$ has an exact $\text{Unif}([0, 1])$ distribution conditional on $T_{-i}(X)$, so it is trivially selectively dominant given $Z = T_{-i}(X)$. For testing $H_0: \theta_1 \leq \theta_{0, i}$, we are in the setting of an MLR family once we condition on $T_{-i}(X)$, so \Cref{exm:mlr} implies that the p-value associated with the UMPU test is also selectively dominant given $Z = T_{-i}(X)$.
\end{example}

\begin{example}[Permutation testing]
In a permutation test we observe data $X \in \mathcal{X}$ and compute a test statistic $T(X)$ that, under the null $H_0$, has a distribution that is invariant under a finite group of transformations $G : \mathcal{X} \rightarrow \mathcal{X}$. That is, $T(X) \overset{d}{=}_{H_0} T(g(X))$ for all $ g \in G$. To run the test, we consider a collection of group elements $g_1, g_2, \dots, g_w$ where $g_1 = id$ is fixed to be the identity transformation and $g_2, \dots, g_w$ are either a random sample from $G$ with replacement or a random sample from $G \setminus \{id \}$ without replacement. The test then rejects when $T(X)$ is large compared to the $T(g_j(X))$. Specifically, the randomized permutation test from Proposition 3 of \cite{Hemerik} uses the p-value
\begin{equation*}
    p = \frac{\#\{1 \leq j \leq w : T(g_j(X)) > T(X) \}}{w} + U_{aux} \frac{\#\{1 \leq j \leq w : T(g_j(X)) = T(X) \}}{w},
\end{equation*}
where $U_{aux} \sim \text{Unif}([0, 1])$ adds auxiliary randomness that is independent of $X$. This p-value always has an exact $\text{Unif}([0, 1])$ distribution under $H_0$ and is therefore is trivially selectively dominant. 
\end{example}

Etablishing \Cref{exm:mlr} and \Cref{exm:exp_fam} is non-trivial, and the bulk of \Cref{sec:one_sided_appdx} is spent doing so. 

\subsection{Example Applications of Selective Dominance}

Having developed our machinery, we provide a few examples that illustrate how to use it.

As an introductory example, we show how to correct for \Cref{exm:publication_bias}'s publication bias. Using our selective dominance machinery, we can provide a one-line derivation of the p-value adjustment from \cite{Hung2020}. \cite{Hung2020} derive this correction specifically for p-values coming from z- and t-tests, but our machinery applies for all selectively dominant p-values. 

\begin{example}[Correcting for publication bias]
\label{exm:correction}
Suppose we have a selectively dominant p-value $p$ for the null hypothesis $H_0$, and we choose to test $H_0$ only after observing that $p \leq \alpha$. We can apply our framework with $p=p$, $Z=0$, and $s(x, z) = I_{x \leq \alpha}$. The selective p-value from \eqref{eq:adjustment} is $p/\alpha$, so \Cref{thm:adjustment} tells us that rejecting when $p \leq \alpha^2$ controls selective Type I error:
\begin{equation*}
    P_{H_0}(p \leq \alpha^2 | S= 1) =  P_{H_0}(p/\alpha \leq \alpha | S= 1) \leq \alpha  
\end{equation*}
\end{example}

As we have learned that essentially all the p-values researchers typically use are selectively dominant, \Cref{exm:correction} gives a simple way for readers to make valid inferences in the presence of publication bias: declare a studies' result significant when the associated p-value is at most $\alpha^2$.

Our rule of thumb of rejecting when $p \leq \alpha^2$ should also deliver valid inferences in the presence of p-hacking. Rather discarding an experiment after observing a p-value larger than $\alpha$, researches more typically tweak their analysis until the p-value crosses the significance threshold. This process, known as p-hacking, is difficult to study theoretically (hence \cite{Hung2020} do not study it). But it has been emprirically well established that, under the null, p-values resulting from p-hacking have left-skewed distributions, i.e., null p-hacked p-values can be reasonably modeled as having an increasing density on $[0,\alpha]$ \citep{Simonsohn}. The transformed p-value $p/\alpha$ then has a density that is increasing on $[0, 1]$ under the null, so \Cref{thm:density} guarantees that it is indeed a valid p-value.

Our second example shows how to use \Cref{thm:adjustment} to perform inference using the ``winning'' p-value. It illustrates how our selective dominance machinery enables us to test data dependent hypotheses, the core problem of selective inference. 

\begin{example}[Inference on the winning p-value]
    \label{exm:winner} Suppose we have $n$ independent and selectively dominant p-values $p_i$ for the null hypotheses $H_{0, i}$, and we choose to test only the $j$th null $H_{0, j}$ after observing that $p_j$ is the smallest of the $p_i$. We will assume that under $H_{0, i}$ each $p_i$ has density that is positive on all of $(0, 1)$. Applying our framework with $p =p_j$, $Z = p_{-j}$, and the selection function $s(x, z) = I_{x < \min_{k} z_k}$, it is straightforward to compute that the adjusted p-value $p_{adj}$ from \eqref{eq:adjustment} is $p_j/\min_{i \neq j} p_i$, so \Cref{thm:adjustment} tells us that rejecting when $p_j \leq \alpha \min_{i \neq j} p_i$ controls selective Type I error:
    \begin{equation}
        \label{eq:winner_error_control}
        P_{H_{0, j}}(p_j \leq \alpha \min_{i \neq j} p_i \mid  p_{-j}, S = 1) \leq \alpha.
    \end{equation} 

    If we let $W$ be the index of the smallest p-value, it is now easy to see that rejecting the data-dependent ``winning'' null $H_{0, W}$ when $p_{(1)} \leq \alpha p_{(2)}$ is controls Type I error both conditionally on $W$ and marginally. Consider only the set of indices $j \in \mathcal{J}$ for which $p_j$ has a positive probability of being the smallest. Conditional error control is immediate: If $H_{0, j}$ is not true, then trivially $P(\text{falsely reject } H_{0, W} \mid W = j) = 0 \leq \alpha$. For the case that $H_{0, j}$ is true, the event $W=j$ is the same event as selecting $p_j$ for inference in \eqref{eq:winner_error_control}, so 
    \begin{align*}
        P(\text{falsely reject } H_{0, W} \mid W = j) &= P(p_{(1)} \leq \alpha p_{(2)} \mid W = j)\\
        &= P(p_j \leq \alpha \min_{i \neq j} p_i \mid W = j)\\
        &\leq \alpha.
    \end{align*}
     Marginal error control follows from the law of total probability. 
    \begin{align*}
        P(\text{falsely reject } H_{0, W}) &= \sum_{j \in \mathcal{J}} P(\text{falsely reject } H_{0, j} \mid W = j)P(W=j) \\
                                          &\leq \alpha \sum_{j \in \mathcal{J}} P(W=j)\\
                                          &\leq \alpha. 
    \end{align*}
    If the nulls are all true and the $p_i$ are exactly uniform, then the inequalities become equalities and our error control is tight. 
\end{example}

Rejecting the null $H_{0, W}$ when $p_{(1)} \leq \alpha p_{(2)}$ may seem like a strange procedure, but we will see that doing so is central to the conditional inference for winners method that arises from \cite{Fithian2017}. In fact, \cite{Fithian2017} implies that, in many settings, this test is UMP amongst tests that are valid conditional on $W$. 

Lastly, we show how our framework also applies to data-carving. Specifically we consider the file-drawer problem from \cite{Fithian2017}.  \cite{Fithian2017} argue that data-splitting, which involves using a chunk of the data for selection and an independent chunk of data for inference, is often an inadmissible approach in selective inference problems. In such settings, data-carving, as we describe below, results in strictly more powerful procedures. Although it initially appears that data-carving's selection procedure does not involve selecting a p-value as we describe in \Cref{sec:dominance}, we show via a coupling argument that it can be viewed in this way. This both serves to illustrate the breadth of our framework's applicability, as well as provide a new perspective on data carving. 

\begin{example}[Data carving and the file-drawer problem]
    \label{exm:carve}
    In the file-drawer problem we observe two independent samples $X_1, X_2 \sim N(\mu, 2)$ (e.g., $X_1$ comes from the first half of the data and $X_2$ from the second). We test the null $H_0: \mu \leq 0$, but only when we observe that $X_1 > t$ for some $t \in \R$.  
    
    Data splitting ignores the first observation, which was used for selection, and simply tests the null with the  p-value $p_{split} = 1 - \Phi(X_2/\sqrt{2}) $. Intuitively, because this p-value is independent of the selection process, we should maintain Type I error control without performing any correction. Applying our framework with $p = p_{split}$, $Z = X_1$, and the selection function $s(x, z) = I(z > t)$, it is not hard to see that the selective p-value indeed offers no correction. 
    
    Data-carving, however, attempts to still use the more powerful p-value $p_{full} = 1 - \Phi( (X_1 + X_2)/2 )$, which leverages information from both samples. How can we apply our framework to this data-carving problem? In how we have stated the problem, it is \underline{not} the case that we observe $p_{full}$ and then decide whether or not to use it for inference. Instead, we decide based on $X_1$, and unlike in data-splitting, $p$ is not a valid p-value given $X_1$. Noting that
    \begin{equation*}
        X_1 | \frac{X_1 + X_2}{2} = y \sim N(y, 1),
    \end{equation*}
    however, we can compute the probability that selection happened given that $p$ took a particular value:
    \begin{equation*}
        P(X_1 > t | p_{full} = x) = 1 - \Phi(t - \Phi^{-1}(1-x)). 
    \end{equation*}
    We may as well therefore imagine that we observed that $p_{full} = x$, and then decided to use it to test the null $H_0$ with probability $1 - \Phi(t - \Phi^{-1}(1-x))$. Although this is not what happens in the original problem (in our new characterization we may test $H_0$ even when $X_1 \leq 3$), the conditional distribution of $p$ given selection is the same in both cases. Hence we can apply our framework with $p = p_{full}$, $Z=0$ and $s(x) = 1 - \Phi(t - \Phi^{-1}(1-x))$. \cite{Fithian2017} argue that the resulting selective p-value
    \begin{equation*}
        p_{carve} = \frac{\int_0^{p_{full}}  1 - \Phi(t - \Phi^{-1}(1-x)) dx}{\int_0^1  1 - \Phi(t - \Phi^{-1}(1-x)) dx} = \frac{\int_0^{p_{full}}  1 - \Phi(t - \Phi^{-1}(1-x))dx }{1 - \Phi(t/\sqrt{2})} 
    \end{equation*}
    will result in strictly more rejections than $p_{split}$. \Cref{sec:carve_appdx} provides more details on the specific calculations done in this example. 
\end{example}

Crucially, in \Cref{exm:carve}, the conditional distribution of the random variable $X_1$ used for selection given the p-value $p_{full}$ did not depend on the unknown parameter $\mu$. Hence, the selection function $s(x)$ had no dependence on $\mu$, and we were able to correct our p-value without any issues. This did not happen by accident, and is actually a consequence of more general and interesting fact regarding the relationship between between data splitting, data carving, data fission \citep{Leiner}, and data thinning \citep{Dharamshi, Neufeld}.

In the most basic version of data fission, we add and subtract independent normal noise $Z \sim N(0, 1)$ to a normal sample $X \sim N(\mu, 1)$ to get two independent samples $X_1, X_2 \sim N(\mu, 2)$ centered at the same mean. This is meant to mimic data splitting: the first sample can be used for selection and the second for inference. Data thinning generalizes this idea to the setting where $X$ is a random vector from a parametric family, and we add noise make $k$ new random vectors $X_1, \dots, X_k$ that (1) are independent and (2) can be used to recover $X$ via a deterministic function $X = T(X_1, \dots, X_k)$. Vanilla data thinning would involve using some of the $X_i$ to perform selection and then the rest to do inference. Data carving, however, suggests using a p-value for inference that is a function of all the data $X = T(X_1, \dots, X_k)$, despite some of the $X_i$ being used for selection. Because the noise we add to $X$ has no dependence on the unknown parameter, the joint distribution of the $X_i$ given $T(X_1, \dots, X_k)$ also has no dependence on the unknown paramter. Therefore, contrary to the what the language in \cite{Leiner} suggests, the selection function $s(x)$ is always \underline{known}, and we can always apply our framework to data carve and get more power. If the selection process is highly complicated, it is true that $s(x)$ may be very difficult to compute, but in theory it is always accessible to us via extensive simulations. 

Our framework also applies to regression problems, including \cite{Lee}'s foundational problem of doing inference on LASSO selected regression coefficients. For sake of brevity, we have moved this discussion to \Cref{sec:lasso_appdx}.  

\Cref{exm:correction}, \Cref{exm:winner}, \Cref{exm:carve} all share a common theme. In all three examples, the practioner cheats. They peak at the p-value and, to varying degrees, they only test the null when the p-value looks promising. The purpose of selective procedures is to adjust the p-value in a way that accounts for this cheating. The harsher the cheating is, the more this adjustment inflates the original p-value. 


\section{Inference on Winners}
\label{sec:winner}

In this section we use our framework to study the inference on winners problem. Along with providing further discussion about \Cref{exm:winner}, we also discuss how hybrid inference \citep{Andrews2023}, which offers a solution to the exploding interval problem, also naturally arises in our framework. For both appproaches, our discussion results in novel interpretations, generalizations, and methods. To be concrete, we will imagine performing inference at level $\alpha = 0.1$ throughout the section.

For now, we focus on the independent data setting. The tools we develop in the next section, however, enable us to do inference on winners when data is generated from a multi-parameter exponential family, which encompasses many correlated data settings. 


\subsection{Conditional Inference}

\begin{figure}[]
    \centering
    \scalebox{1}{
    \hspace{-0.03\textwidth}
    \begin{minipage}{0.33\textwidth}
        \centering
        \includegraphics[width=\textwidth]{tail_prob_1.pdf}
    \end{minipage}\hfill
    \hspace{0.02\textwidth}
    \begin{minipage}{0.33\textwidth}
        \centering
        \includegraphics[width=\textwidth]{tail_prob_2.pdf}
    \end{minipage}\hfill
    \hspace{0.02\textwidth}
    \begin{minipage}{0.33\textwidth}
        \centering
        \includegraphics[width=\textwidth]{tail_prob_3.pdf}
    \end{minipage}
    }
    \caption{We plot the level $\alpha=0.1$ conditional LCB $\hat{\mu}$ for different gaps between the winning value $X_W$ and the runner up value $X_R$ and highlight the distance between $\hat{\mu}$ and $X_W$ in red. The LCB $\hat{\mu}$ is chosen exactly so that the tail probability $P(N(\hat{\mu}, 1) > X_R)$, shaded in blue, is $1/\alpha=10$ times the tail probability $P(N(\hat{\mu}, 1) > X_W)$, shaded in red (the overlap appears purple). As $X_W$ and $X_R$ get closer, we need to take $\hat{\mu}$ further and further back for this condition to be satisfied.}
    \label{fig:tail_prob}
\end{figure}

In this sub-section we discuss \cite{Fithian2017}'s conditional approach for performing inference on winners. This conditional approach turns out to be highly related to the testing procedure that we derived in \Cref{exm:winner}. 

\begin{corollary}[Testing the winner]
    \label{cor:cond}
    Suppose that $p_i$ are $n$ independent and selectively dominant p-values under the nulls $H_{0, i}$, and let $W$ be the index of the smallest p-value. Rejecting $H_{0, W}$ when $p_{(1)} \leq \alpha p_{(2)}$ controls Type I error at level $\alpha$ conditionally on $W$, and therefore also marginally. 
\end{corollary}

Unlike Sidak's simultaneous approach, which rejects the winning null when the smallest p-value is small in absolute terms, the conditional approach rejects the winning null when the smallest p-value is small relative to the second smallest p-value. This procedure is strange, but fairly easy to interpret: we reject the winning null when the most extreme observation is $1/\alpha = 10$ more extreme under its null than the second most extreme observation. This can be quite a stringent requirement! 

Once written in terms of p-values, its easy to mathematically see the merits and pitfalls of the conditional approach. If all the p-values except the smallest provide essentially no evidence against the null,  then $p_{(2)} \approx 1$ and we reject when $p_{(1)} < \alpha$, the same p-value cutoff as a one-dimensional problem. On the other hand, if even just one other p-value provides a similar amount of evidence against the null as the smallest p-value, then then $p_{(2)} \approx p_{(1)}$ and we will never reject because $p_{(1)} \not < \alpha p_{(1)} \approx \alpha p_{(2)}$.

The p-value viewpoint also makes it clear that the conditional approach can only outperform the simultaneous approach when some null p-values are conservative (i.e., they are super-uniform). We give a heuristic argument here, and a more formal statement in \Cref{sec:beta_dist_appdx}. Suppose one of our p-values $p_1$ is a very strong signal (so it is very small with high probability) but the remaining p-values $p_2, \dots, p_n$ are null p-values that are uniform (i.e., they are not conservative). Our conditional procedure will reject when our smallest p-value, likely $p_1$, is less than $\alpha$ times the smallest of $p_2, \dots, p_n$. The minimum of these $n-1$ uniform p-values is $1/n$ on average. Hence, roughly speaking, the conditional approach also rejects when $p_{(1)}$ is less than $\alpha/n$, which is essentially the same as Sidak's simultaneous approach when $n$ is large. 

%In \Cref{fig:sim_results} we imagine observing independent Gaussian data $X_i \sim N(\mu_i, 1)$ and trying to verify the existence of a positive mean by testing the nulls $H^0_{0, i} : \mu \leq 0 $. The simulations affirm that the conditional method performs comparably to Sidak when no nulls are conservative and there is one strong signal, better when some nulls are conservative and there is one strong signal, and poorly when there are multiple strong signals. 

\subsubsection{Confidence regions for the winner}

In parametric problems, we can invert \Cref{cor:cond}'s test to get selective confidence regions for the winning parameter. Consider observing independent data $X_i \sim P_{\theta_i}$ from an MLR family $P_{\theta}$ parametrized by $\theta \in \R$. Let $p_i^{\theta_0}$ (which is a function of $X_i$) be the UMP p-value for testing the null $H_0 : \theta_i \leq \theta_0$. Details regarding these p-values can be found in \Cref{sec:one_sided_mlr_appdx}. We can define the winner $W = \argmin_{j \in [n]} p_j^{\theta_0}$ to be the index of the smallest and most promising p-value. This winning index will be the same irrespsective of $\theta_0$  \footnote{If we use the same auxiliary randomness to compute $p_i^{\theta_0}$ for every $\theta_0$, then one index will result in the smallest p-value for every $\theta_0$ and $W$ is well-defined. The discussion in \Cref{sec:one_sided_mlr_appdx} implies that this will be the case. }. By inverting \Cref{cor:cond}'s test, we get an LCB

\begin{equation}
    \label{eq:winning_lcb}
    \{\theta_0 : p^{\theta_0}_{(1)}/p^{\theta_0}_{(2)} > \alpha  \}
\end{equation}
for the winning parameter $\theta_W$ that holds conditionally on $W$ with probability exactly $1-\alpha$:
\begin{equation*}
    P( \theta_W \in\{\theta_0 : p^{\theta_0}_{(1)}/p^{\theta_0}_{(2)} > \alpha  \} |W ) = 1-\alpha.
\end{equation*}
The fact that the confidence region \eqref{eq:winning_lcb} is actually an LCB is a consequence of the selective p-value $p_{(1)}^{\theta_0}/p^{\theta^0}_{(2)}$ being monotone non-decreasing in null parameter $\theta_0$. \Cref{sec:one_sided_monotone_appdx} provides general conditions under which selective p-values like $p_{(1)}^{\theta_0}/p^{\theta^0}_{(2)}$ are monotone in the null parameter. We show these conditions apply to the winner problem, and also argue that \eqref{eq:winning_lcb} has exact $1-\alpha$ coverage in \Cref{sec:winner_appdx}. We also show in \Cref{sec:winner_appdx} how to invert \Cref{cor:cond}'s test to get a CI (rather than LCB), and that both our CI and LCB match \cite{Fithian2017}'s approach in the Gaussian case. 

Writing the conditional inference in terms of p-values helps us better understand \cite{Fithian2017}'s conditional LCB \eqref{eq:winning_lcb}. In particular, we learn that \Cref{fig:lcb}'s LCB stretches back exactly to the $\hat{\theta}$ under which it is $1/\alpha = 10$ times less likely to see something as extreme as the winner than something as extreme as the runner-up. \Cref{fig:tail_prob} provides an illustration for the Gaussian case. It demonstrates why why the LCB diverges to $-\infty$ as the winner and runner-up get closer. If the winner and runner-up are very close, we will need the LCB $\hat{\mu}$ to be very far back for the winner to be ten times more extreme than the runner-up. Thanks to the decay of the Gaussian tail, however, we can always find such a mean if we go far back enough. 

For non-Gaussian data, the amount the conditional LCB \eqref{eq:winning_lcb} stretches back depends on the tail decay of $P_{\theta}$. The faster the tail decays, the larger the first $\hat{\theta}$ for which the winner is $1/\alpha = 10$ times as extreme as the runner-up. Seeing as the Gaussian distribution, whose tail shrinks as $e^{-x^2}$, still often results in very low lower bounds, we should expect that the distance from the winning observation to the lower bound will often be quite large in many settings.  

As an example, consider observing independent exponential random variables $X_i \sim \text{Exp}(\lambda_i)$. The exponential distribution has a tail $e^{-x}$ that decays fast, but not as fast as the Gaussian tail. Crucially, it also has a parameter space $\lambda \in (0, \infty)$ that is bounded below. The exponential distribution has an MLR in $T(x) = 1/x$, so the UMP test for $H^{\lambda_0}_{0, i}: \lambda_i \leq \lambda_0$ uses a p-value $p^{\lambda_0}_i$ that rejects when $X_i$ is small. It turns out that 
\begin{equation*}
    \lim_{\lambda^0 \downarrow 0} p^{\lambda_0}_{(1)}/p^{\lambda_0}_{(2)} = X_{(1)}/X_{(2)},
\end{equation*}   
so the conditional LCB for the winning parameter $\lambda_W$,
\begin{equation}
\label{eq:exp_winning_lcb}
 \{\lambda_0 : p^{\lambda_0}_{(1)}/p^{\lambda_0}_{(2)}  > \alpha\},
\end{equation}
is vacuous whenever $X_{(1)}/X_{(2)} > \alpha$, i.e., with positive probability the confidence region \eqref{eq:winning_lcb} spans the whole parameter space $(0, \infty)$. A careful derviation of this test and result can be found in \Cref{sec:exponential_winner_appdx}. 

The failure of the conditional LCB \eqref{eq:exp_winning_lcb} manifests in real data examples. On a dataset of car engine failure times \citep{Molotaliev}, we find that the conditional LCB \eqref{eq:exp_winning_lcb} is always vacuous. The dataset has the failure times of one-hundred car engines, which we model as independent exponential random variables. Over many subsamples of just $n=2$ failure times, the LCB \eqref{eq:winning_lcb}, which does inference on the worse of the two engines, always gives a vacuous lower bound of zero. In contrast, the simultaneous approach always gives a non-vacuous lower bound. \Cref{fig:car_engine} depicts the results. The result is concerning. Despite conditional LCB having exact $1-\alpha=0.9$ coverage, our empirical coverage is perfect (the vacuous LCB must always cover the parameter). This suggests that the exponential distribution is likely not an appropriate model for this dataset, even though it is often a natural choice for modeling failure times. 
\begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{car_eng.pdf} % 
    \caption{Over $B=1000$ different subsamples of $n=2$ failure times from the dataset \cite{Molotaliev}, the distribution of the LCB for the ``winning'' parameter resulting from the conditional and simulatneous approaches. The conditional LCB is always vacuous.}
    \label{fig:car_engine}
\end{figure}

\subsubsection{More discoveries via the closure principle}

Once written in terms of p-values, it is natural to treat \Cref{cor:cond}'s test as a test of the global null and try to close it (as in \cite{Marcus}). Closing a global null test precludes us from making confidence regions, but it allows us to make more individual discoveries. Closed global null testing procedures are often computationally intractable to implement, so it is interesting that \Cref{cor:cond}'s global null test admits a tractable closure.  

\begin{corollary}[Closed testing for winners]
    \label{cor:cond_closed}
    Suppose that $p_i$ are $n$ independent and selectively dominant p-values under the nulls $H_{0, i}$. As shorthand, let $H_{0, (j)}$ denote the null corresponding to the $j$th smallest p-value (ties broken randomly) and define $p_{(n+1)} = 1$.  Rejecting the null hypotheses $H_{0, (k)}$ for which $p_{(j)} \leq \alpha p_{(j-1)} $ for every $j \leq k$ controls FWER error at level $\alpha$. 
\end{corollary}

As is often the case for closed procedures, \Cref{cor:cond_closed} procedure is best understood sequentially. We reject $H_{0, (1)}$ if $p_{(1)} \leq \alpha p_{(2)}$. Then, if we rejected  $H_{0, (1)}$, we reject $H_{0, (2)}$ if $p_{(2)} \leq \alpha p_{(3)}$, so on and so forth until we fail to reject. 

\subsection{Hybrid Inference}

Hybrid inference, originally proposed by \cite{Andrews2023}, is an inference on winners procedure that attempts to balance the benefits of the simultaneous and conditional approaches. It is a very elegant idea, but it currently only applies to Gaussian data and can be difficult to parse and implement. Using our selective dominance framework, we give a simpler exposition of hybrid inference that enables its application in more general settings, provided that the data is independent. As a bonus, our new procedure is very easy to undestand and implement.

\Cref{cor:hyb} presents our hybrid testing procedure. We give the sketch of a proof and defer a detailed proof to \Cref{sec:hyb_proof_appdx}. 

\begin{corollary}[Hybrid test for the winner]
    \label{cor:hyb}
    Suppose that $p_i$ are $n$ independent and selectively dominant p-values under the nulls $H_{0, i}$, let $W$ be the index of the smallest p-value, and fix some $\beta < \alpha$. Rejecting $H_{0, W}$ when 
    \begin{equation}
        \label{eq:hybrid_cutoff_thm}
        p_{(1)} \leq \frac{\alpha-\beta}{1-\beta}p_{(2)}  + \left(1 - \frac{\alpha - \beta}{1 - \beta}\right) \beta_n
    \end{equation} 
    controls Type I error at level $\alpha$. 
\end{corollary}

\begin{proof}[Proof sketch]    
    Let $B$ be the event that the smallest p-value comes from a null and is at most $\beta_n$. We know from Sidak's procedure that $P(B) \leq \beta$. Hence, on the complementary event $B^c$, which has probability $\geq 1-\beta$, it suffices to ensure that we fail to falsely reject $H_{0, W}$ with probability at least $(1-\alpha)/(1-\beta)$. Supposing $H_{0, j}$ is true, imagine testing $H_{0, j}$ using $p_j$ only when $B^c$ happens and $W=j$. This is exactly like selecting $p_j$ to use for inference when it is between $\beta_n$ and $\max_{i \neq j} p_i $. For this selection, \Cref{thm:adjustment}'s selective p-value is given by $(p_j - \beta_n)/(\max_{i \neq j} p_i  - \beta_n)$, so we can ensure that we fail to reject $H_{0, j}$ when $B^c$ and $W=j$ happen with probability at least $(1-\alpha)/(1-\beta)$ if we fail to reject whenever
    \begin{equation*}
        \frac{p_j - \beta_n}{\max_{i \neq j} p_i  - \beta_n} > 1 - \frac{1-\alpha}{1-\beta} \iff p_j > \frac{\alpha-\beta}{1-\beta} \max_{i \neq j} p_i + \left(1 - \frac{\alpha-\beta}{1-\beta}\right)\beta_n.
    \end{equation*}
    Our hybrid inference procedure fails to reject in this case.  
\end{proof}

Written in terms of p-values, it is easy to see how the hybrid approach balances the benefits of the simultaneous and conditional approaches. It will reject both when the smallest p-value is small in absolute terms or when it small relative to the second smallest p-value. When the other p-values provide essentially no evidence against the null (i.e., $p_{(2)} \approx 1$), hybrid always rejects when $p_{(1)}$ is less than $(\alpha-\beta)/(1-\beta)$, a cutoff that has no dependence on the problem dimension $n$. In this situation, it performs at least on par with the conditional procedure run at level $(1-\alpha)/(1-\beta)$. On the other hand, even if some other p-value provides as much evidence against the null as the smallest, hybrid still rejects whenever the level $\beta$ simultaneous approach does. This is because when $p_{(1)} \leq \beta_n$, the hybrid cutoff is a mixture of two things that are larger than $p_{(1)}$, and we will reject. 

The parameter $\beta$ allows hybrid inference to interpolate between the simultaneous and conditional approaches. When we set $\beta = 0$ then the hybrid cutoff \eqref{eq:hybrid_cutoff_thm} becomes $\alpha p_{(2)}$ and we recover the conditional method, and if we set $\beta=\alpha$ it becomes $\alpha_n$ and we recover the simultaneous method. 

\subsubsection{Confidence Regions}

In parametric settings, we can get hybrid confidence regions for the winning parameter by inverting \Cref{cor:hyb}'s test. Again suppose we have independent data $X_i \sim P_{\theta_i}$ from an MLR family $P_{\theta}$ parametrized by $\theta \in \R$, and let $p_i^{\theta_0}$ be the UMP p-value for testing the null $H^{\theta_0}_{0, i} : \theta_i \leq \theta_0$. By inverting \Cref{cor:hyb}'s test we get a hybrid LCB for the winning parameter $\theta_W$:
\begin{equation}
    \label{eq:hyb_lcb}
    \left\{ \theta_0 \in \R:  p^{\theta_0}_{(1)} > \frac{\alpha-\beta}{1-\beta}p^{\theta_0}_{(2)}  + \left(1 - \frac{\alpha - \beta}{1 - \beta}\right) \beta_n \right\}.
\end{equation}
We argue in \Cref{sec:hybrid_appdx} that the confidence region \eqref{eq:hyb_lcb} indeed gives a LCB. We also show in \Cref{sec:hybrid_appdx} how to invert \Cref{cor:hyb}'s test to get a CI (rather than an LCB), and that both our CI and LCB match the original construction from \cite{Andrews2023} in the Gaussian case. 

\subsubsection{Comparison to the Union Bound}

\begin{figure}[]
    \centering
    \includegraphics[width=0.55\textwidth]{hyb_dist_to_winner.pdf}
    \caption{For the $n = 20$ dimensional Gaussian problem $X_i \sim N(\mu_i, \sigma^2)$ with largest observation $X_W$ and second largest observation $X_R$, the standardized distance $(X_W - \hat{\mu})/\sigma$ from $X_W$ to the level $\alpha = 0.1$ hybrid, conditional, and simultaneous LCB $\hat{\mu}$ as a function of the standardized gap $(X_W - X_R)/\sigma$ between the winner and runner-up. For hybrid we take $\beta = 0.01$. The larger of the level $\alpha = 0.09$ conditional LCB and level $\alpha = 0.01$ simultaneous LCB are shown in green (i.e., the union
    bound LCB).}
    \label{fig:hybrid}
\end{figure}

Another way to balance the benefits of the conditional and simultaneous approaches is to apply a union bound. Naively, we can reject the winning null whenever the level $\beta$ simultaneous approach rejects or the level $\alpha -\beta$ conditional approach rejects, i.e., whenever
\begin{equation}
    \label{eq:union_bound_cutoff}
    p_{(1)} \leq \max\{(\alpha - \beta) p_{(2)}, \beta_n\}.
\end{equation}
The union bound harshly switches between the simultaneous and conditional approaches, whereas the hybrid approach smoothly interpolates between them. This is illustrated in \Cref{fig:hybrid}, which compares the LCBs resulting from the hybrid versus union bound approaches in the $n=20$ dimensional Gaussian problem. 

Written in terms of p-values, we easily see that the hybrid approach dominates the union bound approach, which is affirmed by \Cref{fig:hybrid}. Both methods reject when $p_{(1)} \leq \beta_n$. But, when $p_{(1)} > \beta_n$, it is easy to verify that the hybrid cutoff \eqref{eq:hybrid_cutoff_thm} will be strictly larger than the union bound cutoff \eqref{eq:union_bound_cutoff}, meaning hybrid will reject whenever the union bound does and more \footnote{the authors \cite{Andrews2023} only point out that hybrid dominates the level $\beta$ classical approach, which is weaker than our statement}. 

Practically speaking, however, hybrid inference does not result in much improvement over the union bound, especially as it pertains to making discoveries. This is already somewhat evident in \Cref{fig:hybrid}, where we see that the hybrid LCB, although always larger than the union bound LCB, is still always very close to it. As the variance $\sigma^2$ gets large, the absolute difference $\hat{\mu}_{hyb} - \hat{\mu}_{union} $ between the hybrid and union bound LCBs grows with $\sigma$, but the relative difference $({\mu}_{hyb} - \hat{\mu}_{union})/\sigma$   (in units of standard deviation) remains the same (see \Cref{sec:hybrid_gap_appdx}). Accordingly, even when the hybrid cutoff \eqref{eq:hybrid_cutoff_thm} is larger than that of the union bound \eqref{eq:union_bound_cutoff}, it is provably not much larger. We detail why in \Cref{sec:hybrid_sim_appdx}, where we also run a number of simulations comparing the power of the hybrid and union bound approaches. In our simulations, we are unable to find a setting where the hybrid approach results in a appreciable power gain.  

Overall, we suggest viewing hybrid inference as a procedure that squeezes the remaining power out of the union bound approach. As it is not computationally more expensive and our p-value viewpoint makes it equally easy to implement, it is always worth using in place of the union bound. 

\subsubsection{Applying the Closure Principal}

Like was true in the conditional case, treating \Cref{cor:hyb}'s test as a global null test and closing it allows us to make more discoveries. As we allow $\beta$ to range from $0$ to $\alpha$, this closed procedure interpolates between \Cref{cor:cond_closed}'s closed procedure and the Holm-Sidak procedure, which is the closure of Sidak's global null test. 

\begin{corollary}[Closed hybrid testing for winners]
    \label{cor:hyb_closed}
    Suppose that $p_i$ are $n$ independent and selectively dominant p-values under the nulls $H_{0, i}$. As shorthand, let $H_{0, (j)}$ denote the null corresponding to the $j$th smallest p-value and define $p_{(n+1)} = 1$. Fixing some $\beta < \alpha$, rejecting the null hypotheses $H_{0, (k)}$ for which
    \begin{equation*}
        p_{(j)} \leq \frac{\alpha - \beta}{1-\beta} p_{(j-1)} + \left(1 - \frac{\alpha - \beta}{1-\beta} \right) \beta_{n - j + 1}   
    \end{equation*}
    for every $j \leq k$ controls FWER error at level $\alpha$. 
\end{corollary}

This closed procedure is also best understood sequentially. We reject $H_{0, (1)}$ if $p_{(1)} \leq \frac{\alpha - \beta}{1-\beta} p_{(2)} + (1 - \frac{\alpha - \beta}{1-\beta}) \beta_n$. Then, if we rejected $H_{0, (1)}$, we reject $H_{0, (2)}$ if $p_{(2)} \leq  \frac{\alpha - \beta}{1-\beta} p_{(3)} + (1 - \frac{\alpha -\beta }{1-\beta}) \beta_{n-1}$, so on and so forth until we fail to reject. 


\section{Rank Verification in Exponential Families}
\label{sec:rank_verification}

In this section we consider the problem of verifying that that the winning parameter is actually larger than the other parameters, i.e., rather than doing inference on the winning parameter, we do inference on the gap between the winning and remaining parameters. Mainly, we give an account of the seminal work \cite{Hung2019} in our selective dominance framework. We show, however, that \cite{Hung2019} do not correctly handle cases where there can be ties for the winner. This is a subtle point, but it is a mistake that is easy to avoid when using our selective dominance framework.

Overall, the section serves to illustrate how our selective dominance machinery provides a straightforward way to correctly design intricate and counter-intuitive selective procedures. For examples of how one may apply these methods, we refer the reader to the original article \cite{Hung2019}, where there are many. 

\subsection{Warm-up: Rank Verification and Type III Error Control}

To motivate the rank verification problem and shed some light on its relationship with selective dominance, we consider a seemingly unrelated classical statistical question about Type III errors. 

A researcher wants to test if the unknown means of two univariate Gaussian samples, $X_1 \sim N(\mu_1, 1/\sqrt{2})$ and $X_2 \sim N(\mu_2, 1/\sqrt{2})$, are different. They end up rejecting the null hypothesis $H_0: \mu_1 = \mu_2$ because the two-sided p-value $2(1 - \Phi(|X_1 - X_2|))$ they learned from introductory statistics is at most $\alpha$. After rejecting, they note $X_1 > X_2$, and claim ``not only are the two means are different, but they must be different because $\mu_1$ is bigger than $\mu_2$''. Your friend, however, only rejected the null that the means are equal. Can they make a claim about the direction of inequality? This is a quesiton of Type III error, and we can use our selective dominance framework to show that the researcher's claim is actually statistically valid. 

Based on the claim, it seems that what the researcher really wants to do is test the one-sided null $H_{0, 12} : \mu_1 \leq \mu_2$ whenever they observe that $X_1 > X_2$, and test the complementary one-sided null $H_{0, 21} : \mu_2 \leq \mu_1$ whenever they observe that $X_2 < X_1$. To test the null $H_{0, ij} : \mu_i \leq \mu_j$ we normally use the UMP p-value $p_{ij} = 1 - \Phi(X_i - X_j)$. In the researcher's case, however, they only select this p-value to use for inference when they observe that $X_i > X_{j}$, or equivalently that $p_{ij} < 1/2$. Since the $p_{ij}$ are selectively dominant, \Cref{thm:adjustment} tells us that, when using $p_{ij}$ to test $H_{0, ij}$, the researcher should correct for this selection and reject when $2p_{ij} \leq \alpha \iff p_{ij} \leq \alpha/2$. Letting $W$ be the index of the winner and $R$ of the runner-up, the final procedure is to reject $H_{0, WR}$ when $p_{WR} \leq \alpha/2$. 

The approach described above verifies the rank of the winning mean with Type I error control, i.e., it affirms not just that the means are different, but that the mean of the winning observation is the larger of the two. The procedure, which rejects when the smaller of the two one-sided p-values is at most $\alpha/2$, is identical to the procedure that rejects when the above two-sided p-value is at most $\alpha$. Hence, for reasons likely unbeknowst to them, the researcher's original claim is indeed statistically valid. We walk through deriving this procedure much more carefully (with as much detail as we did in \Cref{exm:winner}) in \Cref{sec:rank_verification_warm_up_appdx}.

It turns out that, for the $n$-dimensional Gaussian problem (we considered the 2-dimensional problem up to this point), it has long been known that verifying the winner's rank by running a one-sided test comparing the winner and runner-up at level $\alpha/2$ controls Type I error \citep{Gutmann}. \cite{Hung2019} claim further that the test can be run at level $n/(n-1) \cdot \alpha/2$, however this claim is false. By taking $\mu_1 = \mu_2$ and $\mu_3 = \dots = \mu_n = -\infty$, it is not hard to verify that the Type I error one gets by running the test at level $\alpha/2$ is exactly $\alpha$. Hence, the error cannot be inflated any further.  

Interestingly, \cite{Hung2019}'s claim becomes true if, rather than wanting to verify that the winning mean is strictly bigger than the other mean, we want to verify that it is at \underline{least as} big. Again let us focus on the 2-dimensional case. If, instead of testing the null $H_{0, WR}: \mu_W \leq \mu_R$, we test the null $H_{0, WR} :\mu_W < \mu_R$, then we can indeed run the one-sided test comparing the winner and runner-up at level $\alpha$ instead of level $\alpha/2$. Assuming without loss of generality that $\mu_1 \geq \mu_2$, the proof is straightforward:
\begin{align*}
    &P_{\mu_1, \mu_2}(\text{falsely reject } H_{0, WR}) &\\
    & =P_{\mu_1, \mu_2}(p_{21} < \alpha | W = 2)P(W=2)I_{\mu_2 < \mu_1} & \text{(no false rejection when $W=1$ or $\mu_2 =\mu_1$)}\\
    &\leq P_{\mu_1, \mu_2}(2p_{21} < 2\alpha | p_{21} \leq 1/2)\cdot \frac{1}{2} \cdot I_{\mu_2 < \mu_1} & \text{($X_1$ wins w.p. at least $1/2$ )}\\
    &\leq 2 \alpha \cdot \frac{1}{2} \cdot I_{\mu_2 < \mu_1} \leq \alpha. & \text{(selective dominantce)}
\end{align*}

As rejection probabilities are typically continuous in the parameter space, it is counter-intuitive that excluding the boundary of the null makes a tangible difference. But, since the null hypothesis is data-dependent, the false rejection region is a highly discontinuous function of the parameters, which elicits this behavior. For example, under the data-dependent null $H_{0, WR} :\mu_W < \mu_R$, the false rejection region is empty when $\mu_1 = \mu_2$, but whenever $\mu_1$ is even $\epsilon$ larger than $\mu_2$,  the false rejection region $X_2 - X_1 > z_{1-\alpha}$ becomes highly non-trivial. Under the null $H_{0, WR} :\mu_W \leq \mu_R$ the false rejection $|X_1 - X_2| > z_{1-\alpha}$ is as large as possible when $\mu_1 = \mu_2$, which prevents us from inflating the level of the test. Fixing $\mu_2 = 0$, \Cref{fig:warm_up_error_control} considers both nulls $H_{0, WR} :\mu_W < \mu_R$ and $H_{0, WR} :\mu_W \leq \mu_R$ and plots the Type I error of the level $\alpha$ one-sided test comparing the winner and runner up for different values of $\mu_1$. The behavior at the discontinuity $\mu_1=0$ illustrates why we have Type I error control for $H_{0, WR} :\mu_W < \mu_R$  but not for $H_{0, WR} :\mu_W \leq \mu_R$. 

A nice implication of our above discussion is the following surprising fact: if we want to to verify that the winning mean amongst $X_1 \sim N(\mu_1, 1/\sqrt{2})$ and $X_2 \sim N(\mu_2, 1/\sqrt{2})$ is \underline{at least} as large as the other mean, we can run a one-sided test comparing the winner to the runner up at level $\alpha$, i.e., with \underline{no correction}, and still mantain Type I error control. 

\begin{figure}[]
    \centering
    \scalebox{1}{
    \begin{minipage}{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{inequality_error.pdf}
    \end{minipage}\hfill
    \begin{minipage}{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{equality_error.pdf}
    \end{minipage}\hfill
    }
    \caption{For $\mu_2=0$ and different $\mu_1$, the type I error of rejecting $H_{0, WR}: \mu_W - \mu_R < 0$ and  $H_{0, WR}: \mu_W - \mu_R \leq 0$ when the level $\alpha$ one-sided test comparing the winner of $X_1 \sim N(\mu_1, 1/\sqrt{2})$ and $X_2 \sim N(\mu_2, 1/\sqrt{2})$  to the runner-up rejects.}
    \label{fig:warm_up_error_control}
\end{figure}


\subsection{Rank Verification in Exponential Families}

In this sub-section we illustrate how to do rank verification when we observe data $X \in \R^n$ from a natural multiparameter exponential family $P_{\theta}$ with density
\begin{equation}
    \label{eq:exp_fam}
    g_{\theta}(x) = \exp(\theta_1 x_1 + \dots + \theta_n x_n - \psi(\theta))g(x),
\end{equation}
with repsect to some base measure. \footnote{We assume that $g(x)$ is symmetric and $T_i(x) = x_i$ so that our discussion more closely mirrors \cite{Hung2019}, although we do not need to. \cite{Hung2019} also assume that $g(x)$ is Schur concave for other purposes, but we leave this assumption out.}. Like \cite{Hung2019}, we want to verify that $\theta_W$ is larger than the remaining $\theta_j$, where $W$ is the index of the largest $X_i$. In case of ties, we follow \cite{Hung2019}'s lead, and set $W$ to randomly be one of the winning indices. Since $E_{\theta}[X_i] = \theta_i$, the winning parameter $\theta_W$ may reasonably be the largest of the $\theta_i$. 

Considering the nulls $H_{0, ij} : \theta_i \leq \theta_j$, we want to reject the data-dependent null $\cup_{j \neq W} H_{0, Wj} $ and affirm that $\theta_W$ larger than any other parameter \footnote{If we performed the same analysis for the nulls $H^{\delta}_{0, ij} : \theta_i  - \theta_j \leq \delta$ then we could verify that $\theta_W$ is more than $\delta$ larger than any other $\theta_j$. Inverting these tests would result in a LCB for the difference $\theta_W - \max_{j \neq W } \theta_j$ between the winning and next largest parameter.}. Our strategy will be to come up with a valid test for $H_{0, Wj}$ for all $j \neq W$, and then reject $\cup_{j \neq W} H_{0, Wj}$ whenever we reject the tests $H_{0, Wj}$ for all $j \neq W$. Fixing $i \neq j$, we start by constructing the UMPU p-value $p_{ij}$ for testing $H_{0, ij}: \theta_i - \theta_j \leq \delta$. Ultimately, we will only use this p-value to test $H_{0, ij}$ when $i$ is selected as our winning index, and we will correspondingly adjust the p-value to account for this selection. 

Defining the transformed sufficient statistics $Y \in \R^n$ by
\begin{equation}
    \label{eq:reparam}
    Y_i = \frac{X_i - X_j}{2}, \qquad  Y_j = \frac{X_i + X_j}{2}, \qquad  Y_{\ell} = X_{\ell} \text{ for } \ell \neq i, j,
\end{equation}
the random vector $Y$ has an exponential family density given by \eqref{eq:exp_fam} as 
\begin{equation}
    \label{eq:exp_fam}
    \tilde{g}_{\theta}(y) = \exp\left( (\theta_i - \theta_j) y_i + (\theta_i + \theta_j) y_j + \sum_{\ell \neq i, j} \theta_{\ell} y_{\ell} - \psi(\theta)  \right)\tilde{g}(y)
\end{equation}
with respect to some other base measure. It is then well established (see \Cref{sec:one_sided_mlr_appdx}) that if we denote the conditional left-continuous survival function of $Y_i$ and its righthand limit as
\begin{equation}
    G_{ij}(y_i | y_{-i}) = P_{\theta_i = \theta_j}(Y_i \geq y_i | Y_{-i} = y_{-i}) \qquad G_{ij}^+(y_i |y_{-i}) = \lim_{u \downarrow y_i } G_{ij}(u | y_{-i}),
\end{equation}
then the UMPU p-value $p_{ij}$ for testing $H_{0, ij}: \theta_i \leq \theta_j$ is given by 
\begin{equation}
    \label{eq:umpu_rank_verification}
    p_{ij} = G^+_{ij}(Y_i | Y_{-i}) + U_{ij, aux}(G_{ij}(Y_{i}|Y_{-i}) - G^+_{ij}(Y_i|Y_{-i})),
\end{equation}
where $U_{ij, aux}$ are $\text{Unif}([0, 1])$ random variables that are independent from each other and the data. By \Cref{exm:exp_fam}, the p-value $p_{ij}$ is selectively dominant given $Y_{-i}$. 

Crucially, we can tell if $X_i$ is a winner by exmaining the p-value $p_{ij}$. It is straightforward to confirm that $X_i$ is the sole winner exactly when $Y_i > \max_{k \neq i } Y_k - Y_j $. Equivalently, this happens when $p_{ij}$ is strictly smaller than 
\begin{equation}
    \label{eq:rank_verification_lower}
    q^+_{ij}(Y_{-i}) = G^+_{ij}(\max_{k \neq i} Y_k - Y_j | Y_{-i}).
\end{equation}
Likewise, one can confirm that $X_i$ is one of multiple winners exactly when $Y_i = \max_{k \neq i } Y_k - Y_j$, or equivalently when $p_{ij}$ is at least $q^+_{ij}$ but at most 
\begin{equation}
    \label{eq:rank_verification_upper}
    q_{ij}(Y_{-i}) = G_{ij}(\max_{k \neq i} Y_k - Y_j | Y_{-i}).
\end{equation}
Moreover, in the case that there are multiple winners, the number of winners is also a deterministic function of $Y_{-i}$:
\begin{equation}
    \label{eq:rank_verification_num_ties}
    N_{i}(Y_{-i}) = 1 + | \{ \ell \neq i : Y_{\ell} = \max_{k \neq i} Y_k  \} |.
\end{equation}
Note that $N_{i}(Y_{-i})$, which is always at least two, is \underline{not} the same as the number of winners, which can be one. Rather, it is the number of winners there will be if $X_i$ is a winner and at least one other $X_k$ is as well (see \Cref{sec:ties_appdx} for details).  

Leveraging these facts, we use selective dominance framework to come up with a valid test for $H_{0, Wj}$. Essentially, we use $p_{ij}$ to test $H_{0, ij}$ with probability one when it is less than $q^+_{ij}$, and with probability $1/N_i$ (we randomly select one of the $N_i$ winners) when it is between $q^+_{ij}$ and $q_{ij}$. Explicitly, letting $p = p_{ij}$ and $Z = Y_{-j}$, we can apply our framework with the selection function
\begin{equation*}
    s(x, z) = 
    \begin{cases} 
    1 & \text{if } x < q_{ij}^+(z), \\
    \frac{1}{N_i(z)} & \text{if } x \in [q_{ij}^+(z), q_{ij}(z)] \\
    0 & \text{otherwise}
    \end{cases}.
\end{equation*}
This is a piece-wise linear function that is easy to integrate, and computations detailed in \Cref{sec:rank_verficiation_adj_appdx} show that the corresponding selective p-value from \Cref{thm:adjustment} is given by 
\begin{equation}
    \label{eq:rank_verification_selective_p_val}
     \frac{p_{ij} - \left(1 - \frac{1}{N_i} \right)(p_{ij} - q^+_{ij})_+ }{q^+_{ij} + \frac{1}{N_i}(q_{ij} - q^+_{ij}) }.
\end{equation}

The crucial difference between our derivation and \cite{Hung2019}'s is that \cite{Hung2019} use the same selective p-value when there can and cannot be ties amongst the $X_k$. If there cannot be ties amongst the $X_k$, then $q_{ij} = q^{+}_{ij}$ always, and the selective p-value \eqref{eq:rank_verification_selective_p_val} simplifies to $p_{ij}/q_{ij}$. If we use $p_{ij}/q_{ij}$ when ties are possible, however, we will not achieve conditional error control (as is claimed in \cite{Hung2019}). We give an example in \Cref{fig:error_control}, where $X \in \R^3$ is composed of three independent binomials. The left panel of \Cref{fig:error_control} depicts the conditional distribution of $p_{12}$ given $W=1$ for a specific setting of the nusiances statistics $Y_{-j}$. It makes it clear that rejecting when $p_{ij}/q_{ij} \leq \alpha $ does not maintain conditional error control, as is affirmed in \Cref{fig:error_control}'s right panel.


\begin{figure}[]
    \centering
    \scalebox{1}{
    \begin{minipage}{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{p_val_density.pdf}
    \end{minipage}\hfill
    \raisebox{0.18cm}{
    \begin{minipage}{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{type_one_error.pdf}
    \end{minipage}\hfill
    }}
    \caption{Considering three independent binomials $X_i \sim \text{Bin}(b, s_i)$ with $b=4$ and $s_i = 1/2$, the first panel (left) depicts $N=10^6$ draws from the conditional distribution of the p-value $p_{12}$ (used for testing $H_{0, 12}: s_1 \leq s_2$) given $W = 1$ and the nuisance statistics $(X_1 + X_2)/2 = 2$, $X_3=2$. When $p_{12}  < q^+_{12}$, then $X_1$ is the sole winner and the p-value is selected for inference with probability one, but when $p_{12}^0 \in [q^+_{12}, q_{12}]$ there is a three-way tie and it is selected for inference with probability $1/3$. Hence, the p-value's conditional distribution is not uniform on $[0, q_{12}]$ as \cite{Hung2019} implicitly assume. The next panel (right) displays the consequence. Conditional on $W=1$, \cite{Hung2019} do not maintain Type I error control when testing $H^{0}_{W2}$ at level $\alpha=0.1$ (denoted by horizontal dashed line), whereas our method does. Error bars denote a 99\% confidence interval.}
    \label{fig:error_control}
\end{figure}

\section{Combining selective p-values}
\label{sec:multiple}

In this section we illustrate how our selective dominance allows us to combine inferences across many p-values, even post-selection. Rather than just selecting just one p-value as in \Cref{sec:dominance}, some of this section's methods select multiple p-values to use for inference. As such, we need to slightly generalize our framework from \Cref{sec:dominance}. The generalization is intuitive, and for sake of brevity we have deferred a formal account of it to \Cref{sec:multiple_p_vals_appdx}. The validity of the methods we propose in this section are a direct consequence of the discussion in  \Cref{sec:multiple_p_vals_appdx}'s. 

\subsection{Publication bias corrected meta-analysis}

\begin{figure}
    \centering
    \includegraphics[width=0.7\textwidth]{replication.pdf} % 
    \caption{Scatter plot of the original $p_O$ and replication $p_R$ p-values for 92 pyschology studies from the open science collaboration's replication analysis \cite{OSF}. Note that the x-axis, which ranges from $[0, 0.05]$ is on a different scale than the y-axis, which ranges from $[0, 1]$. }
    \label{fig:replication}
\end{figure}

Performing replication studies is a crucial part of the scientific process, especially when one is wary that the original study may suffer from publication bias. To judge the prevalence of publication bias in pyschology, the open science collaboration conducted a mass replication analysis of pyschology studies \citep{OSF}. Via their efforts, we have access to p-values from 92 pairs of original and replication pyschology studies \footnote{We exclude seven studies whose original p-value $p_O$ is larger than $\alpha=0.05$.}, depicted in \Cref{fig:replication}. We refer to p-values from the original study as $p_O$ and p-values from the replication study as $p_R$. The p-values $p_O$ from all the original studies are significant at the $\alpha=0.05$ level, while only 34 of the replication p-values $p_R$ are significant. 

Although the original study p-values suffer from publication bias, they still contain valuable and usable information. By \Cref{exm:correction} and its subsequent discussion, $p_O/\alpha$ should still be a valid p-value despite any publication bias (or p-hacking). Via Fisher's combination test, we can use both the corrected original p-value $p_O/\alpha$ and uncorrected replication p-value $p_R$ for inference. As a refresher, Fisher's test considers $n$ independent p-values $p_i$ for the nulls $H_{0, i}$ and rejects the global null $\cap_{i=1}^n H_{i, 0}$ when the test statistic $-2 \sum_{i=1}^n \log(p_i) $ is at least as large as the $1-\alpha$ quantile of the $\chi^2_{2n}$ distribution. In our case, we have two independent p-values $p_O/\alpha$ and $p_R$ that test the same null hypothesis, and we can reject this null hypothesis when 
\begin{equation*}
    -2 (\log(p_O/\alpha) + \log(p_R)) \geq \text{Quantile}(1-\alpha, \chi^2_4). 
\end{equation*}
This approach is analogous to data-carving. We use part of our data for selection (the original study) and part for inference (the replication study), but we still use the information remaining in the the first part after selection for inference as well. Unlike existing approaches to data-carving, which are often complex and problem specific, Fisher's combination test in junction with our selective dominance framework provides a general and simple way to data-carve. 

On the open science dataset, our combination approach allows us to make more powerful inferences from the replication studies. Our approach finds that 47 study pairs have signicant findings, whereas solely using the original study p-value $p_O/\alpha$ or the replication study p-value $p_R$ results in only 39 or 34 significant findings respectively. It is suprising that, even after a harsh adjustment for publication bias, the corrected original study p-values result in more discoveries than replication study p-values. It is hard to gauge if this is due to chance, differences between the original and replication studies (e.g., minor differences in population demographics, devices used for measurement, sample size), or because there is somehow even harsher selection bias in the original studies than what we have accounted for. It may even be possible that some replicators felt incentivized to induce bias in the opposite direction, and tried to ensure that the replication studies were \underline{not} significant. 

\subsection{Adaptive Versions of Fisher's Combination Test}

\begin{figure}
    \centering
    \hspace{-0.035\textwidth}
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{fig/fisher_ell=3.pdf}
        \caption*{(a) $\ell=3$}
    \end{minipage}
    \hfill
    \hspace{0.01\textwidth}
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{fig/fisher_ell=5.pdf}
        \caption*{(b) $\ell=5$}
    \end{minipage}
    \hfill
    \hspace{0.01\textwidth}
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{fig/fisher_ell=10.pdf}
        \caption*{(c) $\ell=10$}
    \end{minipage}
    \caption{ For $\ell=3$ (left), $\ell=5$ (middle), and $\ell=10$ (right), power of the top $k=3$ conditional, $\tau=0.5$ truncated, and original Fisher's combination test for data drawn from $N(\mu, I_{10})$ with $\mu_{1}= \dots = \mu_{\ell}$ varying according to the x-axis and $\mu_{\ell + 1} = \dots = \mu_n = -2$. Power results from an average over $N=10^4$ trials, bands denote one standard error (barely visible), and the level $\alpha=0.1$ is denoted by the dashed line.}
    \label{fig:fisher}
\end{figure}

By employing similar ideas to the previous sub-section, we can come up with variants of Fisher's combination test that are more powerful when some null p-values are conservative (i.e., super-uniform). \Cref{cor:cfisher}, which is of similar flavor to the conditional inference on winners procedure, gives a conditional version of Fisher's combination test that only uses the top $k$ p-values for inference. For this procedure, $k$ must be fixed beforehand. 

\begin{corollary}[Fisher's top-$k$ combination test]
    \label{cor:cfisher}
    Suppose that $p_i$ are $n$ independent and selectively dominant p-values under the nulls $H_{0, i}$ and let $H_{0, (j)}$ denote the null corresponding to the $j$th smallest p-value. For some fixed $1 \leq k \leq n$,  rejecting the data-dependent global null $\cap_{j=1}^k H_{0, (j)}$ (and therefore also the global null $\cap_{i=1}^n H_{0, i}$) when 
    \begin{equation}
        \label{eq:fisher_conditional}
        -2 \sum_{j=1}^k \log(p_{(j)}/p_{(k+1)}) \geq \text{Quantile}(1-\alpha, \chi^2_{2k})
    \end{equation}
    controls Type I error at level $\alpha$ conditional on the indicies of the smallest $k$ p-values (and therefore also marginally). 
\end{corollary}

Examining \eqref{eq:fisher_conditional}, we see that when the $(k+1)$st p-value provides essentially no evidence against the null (so $p_{(k+1)} \approx 1$), running \Cref{cor:cfisher}'s test is like running Fisher's test using just the top $k$ p-values and ignoring that any selection took place. In this case, our test statistic will have an essentially identical value to Fisher's original test statistic, but the critical value required for rejection will be much smaller. On the flipside, if many of the $p_{(j)}$ for $j \leq k$ are not sufficiently smaller than $p_{(k+1)}$, then \Cref{cor:cfisher}'s test statistic will be small and the test will fail to reject.

Another approach to improving Fisher's combination test is truncation: only use a p-value for inference if it is below some fixed threshold $\tau \in \R$ \citep{Zaykin}. Past and previous work, however, only establishes validity of this test when the null p-values have exact $\text{Unif[0, 1]}$ distributions \cite{Zaykin, Zhang}. \Cref{cor:tfisher}, however, gives a version of Fisher's truncated combination test that is still valid whenever the p-values are independent and selectively dominant \footnote{We give a variant that is valid conditional on which p-values are selected. If we just care about rejecting the global null $\cap_{i=1}^n H_{0, i}$ we can give a more powerful test via marginalization, as \cite{Zaykin} do.}

\begin{corollary}
    \label{cor:tfisher}
    Suppose that $p_i$ are $n$ independent and selectively dominant p-values under the nulls $H_{0, i}$ and fix $n$ thresholds $\tau_i \in [0, 1]$. Letting $j \in J$ denote the random set of indices for which $p_j \leq \tau_j$, rejecting the data-dependent global null $\cap_{j \in J} H_{0, j}$ (and therefore also the global null $\cap_{i=1}^n H_{0, i}$) when 
    \begin{equation*}
        -2 \sum_{j=1}^k \log(p_j/\tau_j) \geq \text{Quantile}(1-\alpha, \chi^2_{2|J|})
    \end{equation*} 
    controls type I error at level $\alpha$ conditional on $J$ (and therefore also marginally). 
\end{corollary}

If some $p_j$ are substantially lower than their truncation point $\tau_j$ but most are above it, then \Cref{cor:tfisher} will be powerful. In this case, \Cref{cor:tfisher}'s test will have a slightly smaller statistic compared to Fisher's original combination test but a much smaller critical value. Hence, the truncated Fisher test is most powerful when some p-values come from strong alternatives but many come from conservative nulls. As such, \Cref{cor:tfisher} actually generalizes the truncated Fisher test to the settings where it is most applicable. On the flipside, if most of the $p_j$ are below $\tau_j$, the truncated test statistic pays a penalty due to selection while its critical value remains essentially unchanged compared to Fisher's original test. 

To illustrate the benefits and drawbacks of these methods, we display their power alongside that of Fisher's original test for a simple $n=10$ dimensional Gaussian problem, where we sample $X \sim N(\mu, I_n)$ and use the p-values $p_i = 1 - \Phi(X_i)$ try and detect the existence of a positive mean. For $\ell \in \{3, 5, 10\}$, we vary the strength $\mu_1 = \dots = \mu_{\ell} > 0$ of our signals and set $\mu_{\ell + 1} = \dots = \mu_n = -2$ to be conservative nulls. We do inference on the top $k=3$ p-values using the conditional version of Fisher's method and set the truncation $\tau = 0.5$ for the truncated version (i.e., we include $p_i$ for which $X_i > 0$). The results are displayed in \Cref{fig:fisher}.

As expected, the new methods outperform Fisher's original method when conservative nulls are present. When $\ell=3$ and the bottom three p-values are much smaller than the rest, the conditional method does incredibly well. But its performance quickly degrades when $\ell = 5, 10$ and the fourth smallest p-value becomes close to the bottom three. The truncated method is more robust, and still considerably improves power when $\ell=5$. Unsurprisingly, both selective methods perform worse than Fisher's original method when $\ell=10$ and every $\mu_i$ is a signal. 


\section*{Acknowledgements}
I would like to thank John Cherian, Yash Nair, Will Hartog, Trevor Hastie, Jonathan Taylor, and James Yang for helpful discussions.

\bibliographystyle{plainnat}
\bibliography{bibliography.bib}

\begin{appendix}

\section{Additional Derivations, Details, and Comments}

\subsection{Comparing Conditional and Sidak Global Null Testing}
\label{sec:beta_dist_appdx}

We consider a setting where we have $n$ independent and selectively dominant p-values $p_1, \dots, p_n$ that are all anti-conservative, i.e.,  $p_j \preceq \text{Unif}([0, 1])$. At worst, these p-values are exact uniforms (e.g., they come from the boundary of the null). 

We will show that, on an event with probability at least $1-\epsilon$, the conditional procedure, which rejects when $p_{(1)} \leq \alpha p_{(2)}$, can only reject if $p_{(1)} \leq C_{\epsilon}/n$ for some constant $C_{\epsilon} > 0$. Hence, without conservative nulls, the conditional approach behaves roughly on the same order as the classical approach (Sidak).   

Letting $U_1, \dots, U_n$ be independent $\text{Unif}([0, 1])$ random variables, two facts are clear. First that $U_{(2)} \sim \text{Beta}(2, n-1)$ has mean $\frac{2}{n + 1} < \frac{2}{n}$ and standard deviation $\sqrt{\frac{2(n-1)}{(n+1)^2(n+2)}} \leq \frac{2}{n}$, and second that $p_{(2)} \preceq U_{(2)}$. 

Fix any $\epsilon > 0$. We have by Chebyshev's inequality that  
\begin{align*}
    P\left(p_{(2)} \leq \frac{2}{n}(1 + \epsilon^{-\frac{1}{2}})\right) &\geq P(p_{(2)} \leq E[U_{(2)}] + \sqrt{\Var(U_{(2)})}/\sqrt{\epsilon})\\
    &\geq P(U_{(2)} \leq E[U_{(2)}] + \sqrt{\Var(U_{(2)})}/\sqrt{\epsilon})\\
    &\geq P(|U_{(2)} - E[U_{(2)}]| < \sqrt{\Var(U_{(2)})}/\sqrt{\epsilon} ) \\
    &\geq 1 - \epsilon
\end{align*}
Define $C_{\epsilon} = 2(1 + \epsilon^{-\frac{1}{2}})\alpha$ and the event $A_{\epsilon} = \{ p_{(2)} \leq \frac{2}{n}(1 + \epsilon^{-\frac{1}{2}})\}$. The event $A_{\epsilon}$ has probability at least $1-\epsilon$, and on this event a the conditional procedure never rejects when the procedure $p_{(1)} > C_{\epsilon}/n$, completing our claim. 

In other words, the conditional procedure can only reject when $p_{(1)} \leq C_{\epsilon}/n$ (except for a small probability event) and hence suffers the same curse of dimensionality as the classical method:
\begin{equation*}
    P(p_{(1)} \leq \alpha p_{2} \text{ and } p_{(1)} > C_{\epsilon}/n) \leq P(A_{\epsilon}^c) \leq \epsilon.  
\end{equation*}


\subsection{Conditional Inference on Winners}
\label{sec:cond_appdx}

\subsubsection{Standard Derivation}

Suppose we observe independent Gaussian data $X \sim N(\mu, I_n)$ and want to make a LCB for the mean $\mu_W$ of the winner $W = \argmax_{i \in [n]} X_i$. 

The conditional approach tries to avoid the classical approach's curse of dimensionality by providing inferences for only the winning mean. It does so by delivering an LCB for $\mu_W$ that is valid conditionally on $W$. To construct the conditional LCB, we follow the framework of \cite{Fithian2017}. Letting $R$ be the index of the runner-up (second largest observation), we note that the deviation of $X_{W}$ from $\mu_{W}$ has a truncated normal distribution once we condition on $W$ and $X_{-W}$:
\begin{equation}
    \label{eq:cond_dist}
    X_W - \mu_W \mid W, X_{-W} \sim TN(0, 1, X_{R} - \mu_{W}, \infty).
\end{equation}
Letting 
\begin{equation}
    \label{eq:cond_quantile}
    q_{1-\alpha}(w, x_{-w}, \mu_w) = \text{Quantile}_{\mu}(1-\alpha, X_W -  \mu_{W} \mid W=w, X_{-w} = x_{-w})
\end{equation}
denote the $1-\alpha$ quantile of this conditional distribution \eqref{eq:cond_dist}, it is straightforward to show that
\begin{equation}
\label{eq:cond_set}
     C_{cond}(X) = \{\eta : \eta > X_{W} - q_{1-\alpha}(W, X_{-W}, \eta)  \} 
\end{equation}
is a $1-\alpha$ confidence region for $\mu_{W}$ that is valid  conditional on $W$ and  $X_{-W}$:
\begin{equation*}
    P_{\mu}( \mu_{W} \in C_{cond}(X) | W, X_{-W})  = P_{\mu}( X_W - \mu_W < q_{1-\alpha}(W, X_{-W}, \mu_{W}) |W, X_W) = 1-\alpha.
\end{equation*}
Since $\eta$ appears on both sides of the membership condition in \eqref{eq:cond_set}, it is not clear that $C_{cond}(X)$ actually provides a lower bound for $\mu_{W}$. We will see, however, that we can rewrite $C_{cond}(X)$ as 
\begin{equation}
\label{eq:cond_lcb}
     C_{cond}(X) = \{\eta : \eta > X_{W} - L(X_W - X_R)  \}. 
\end{equation}
where $L: \R \rightarrow \R$ is a complicated ``length'' function that determines the distance from the winner $X_W$ to the lower confidence bound. We will de-mistify $L(\cdot)$ in the next sub-section. For now, we provide a plot of it in the middle panel of \Cref{fig:lcb}.

As can be seen in \Cref{fig:lcb}, the conditional LCB has very interesting behavior. In some situations, it indeed seems to avoid the curse of dimensionality. If the gap between $X_W$ and $X_R$ is large, then the conditional LCB for $\mu_W$ will be roughly $X_{W} - z_{1-\alpha}$, i.e, what we expect in a one-dimensional inference problem. But the cost of using the conditional approach can be tremendous. As the runner-up gets close to the winner, the conditional LCB goes quickly to $-\infty$ and can give much worse inferences than the classical approach. For example, if the runner-up is within just one standard deviation of the winner, we get inferences similar to those in the $n = 100$ dimensional classical problem. 

Compared to the classical approach, which provides a lower bound that is a fixed distance from the winner, the conditional approach is much harder to interpret. Its quality depends on a somewhat opaque relationship between the runner-up and the winner. How separated does the winner need to be from the runner-up for the conditional approach to be useful? And how close can the winner and runner-up be before the classical approach is better? We shed light on these questions by rewriting the conditional procedure in terms of p-values. 

\subsubsection{p-Value Viewpoint }
\label{sec:cond_appdx}

Considering data $X \sim N(\mu, I_n)$ with unknown mean $\mu$ and the p-values $p^{\mu_0
}_i = 1 - \Phi(X_i - \mu_0)$, we want to characterize when the conditional LCB is at least $\mu_0 \in \R$. This happens exactly when $\mu_0$ is not included in the set \eqref{eq:cond_set}. Examining \eqref{eq:cond_dist}, \eqref{eq:cond_quantile}, and \eqref{eq:cond_set}, this happens when $X_W - \mu_0$ is at least as large as the $1-\alpha$ quantile $Q$ of a standard normal truncated to be larger than $X_R - \mu_0$. This quantile satisfies 
    \begin{equation*}
        \alpha = \frac{1 - \Phi(Q) }{1 - \Phi(X_R - \mu_0) }.
    \end{equation*}
Solving for $Q$ gives $Q = \Phi^{-1}(1 - \alpha(1 - \Phi(X_R - \mu_0)))$, meaning we reject exactly when 
\begin{align*}
    X_{W} - \mu_0 \geq \Phi^{-1}(1 - \alpha(1 - \Phi(X_{R} - \mu_0))) &\iff 1 - \Phi(X_{W} - \mu_0) \leq \alpha(1 - \Phi(X_{R} - \mu_0))\\
    &\iff p^{\mu_0}_{(1)} \leq \alpha p^{\mu_0}_{(2)}.
\end{align*}

\subsubsection{Distance Between $X_W$ and Conditional LCB }
\label{sec:gap_appdx}

Consider observing Gaussian data $X \sim N(\mu, I_n)$ and let $W$ and $R$ be the indicies of the winner and runner up respectively. We briefly justify that the distance between $X_W$ and the conditional LCB for $\mu_W$ depends only on the gap between $X_W - X_R$. Define $D = X_W - \mu_0$ to be distance between $X_W$ and the conditional LCB $\hat{\mu}$. The conditional LCB $\hat{\mu}$ satisfies 
\begin{equation*}
    \frac{p^{\hat{\mu}}(X_W)}{p^{\hat{\mu}}(X_R)} = \alpha \iff \frac{1 - \Phi(X_W - \hat{\mu})}{1 - \Phi(X_R - \hat{\mu})} = \alpha \iff \frac{1 - \Phi(D)}{1 - \Phi(D - (X_W - X_R))} =\alpha.
\end{equation*}
Clearly then $D$ is a function of $X_W - X_R$.

\subsubsection{The selective p-value }
coming soon

\subsubsection{Confidence lower bound}
coming soon

\subsubsection{Confidence interval}
coming soon

\subsection{Hybrid Inference on Winners}
\label{sec:hybrid_appdx}

\subsubsection{Standard Deviation}

Like before, we want to make a LCB for the mean $\mu_{W}$ of the winner $W = \argmax_{i \in [n]} X_i$ in the case of independent Gaussian data $X \sim N(\mu, I_n)$ with unknown mean $\mu \in \R^n$. 

The core idea behind hybrid inference is giving a confidence region $C_{hyb}(X)$ that has a very high probability of containing $\mu_W$ on a ``good'' event $G_{\mu}$. Oddly, this good event depends on the unknown parameter. For some $\beta < \alpha$, we need $G_{\mu}$ to happen with probability at least $1-\beta$. Then, if we ensure that $C_{hyb}(X)$ has at least $(1-\alpha)/(1-\beta)$ coverage on the $G_{\mu}$, it will achieve $1-\alpha$ coverage overall:
\begin{align*}
       P_{\mu}(\mu_{W} \in C_{hyb}(X)) &= P_{\mu}(G_{\mu})P_{\mu}(\mu_{W} \in C_{hyb}(X) | G_{\mu}) + P_{\mu}(G_{\mu}^c)P_{\mu}(\mu_{W} \in C_{hyb}(X) | G_{\mu}^c)\\
       &\geq (1-\beta)\left(\frac{1-\alpha}{1-\beta} \right)\\
       &=1-\alpha.
\end{align*}

Considering some $\beta < \alpha$ and defining $\beta_n = 1 - (1-\beta)^{\frac{1}{n}}$ as in \eqref{eq:bonf_quantile}, our good event is that the confidence lower bounds $X_i -  z_{1-\beta_n}$ for the means $\mu_i$ all simultaneously hold:
\begin{equation*}
    G_{\mu} = \{X_i < \mu_i + z_{1-\beta_n} \text{ for all } i \in [n] \}.
\end{equation*}
From our earlier reasoning, we know that this good event happens with probability exactly $1-\beta$. 

Now, we can make a confidence region that contains the mean with probability at least $(1-\alpha)/(1-\beta)$ on this good event. If we condition on $G_{\mu}$ along with $W$ and $X_{-W}$, the deviation of $X_W$ from $\mu_W$ has a truncated normal distribution like \eqref{eq:cond_dist} that is further truncated from above:
\begin{equation}
    \label{eq:hyb_dist}
    X_W - \mu_W \mid W, X_{-W}, G_{\mu} \sim TN(0, 1, X_{R} - \mu_{W}, z_{1-\beta_n} ).
\end{equation}
On the good event $G_{\mu}$, we always have that $X_R - \mu_{W} < X_W - \mu_{W} < z_{1-\beta_n}$, so the lower truncation is indeed below the upper one. Let
\begin{equation}
\label{eq:hyb_quantile}
    q^{h}_{\frac{1-\alpha}{1-\beta}}(w, x_{-w}, \mu_w) = \text{Quantile}_{\mu}\left(\frac{1-\alpha}{1-\beta}, X_W -  \mu_{W} \mid W=w, X_{-w} = x_{-w}, G_{\mu}\right)
\end{equation}
denote the $(1-\alpha)/(1-\beta)$ quantile of the conditional distribution \eqref{eq:hyb_dist}. Per the prior discussion, the function \eqref{eq:hyb_quantile} only makes sense if the largest value in $x_{-w}$ at most $z_{1-\beta_n}$, and we will take the quantile \eqref{eq:hyb_quantile} to be $-\infty$ if it is not. It is then straightforward to show that
\begin{equation}
\label{eq:hyb_set}
     C_{hyb}(X) = \{\eta : \eta > X_{W} - q^{h}_{\frac{1-\alpha}{1-\beta}}(W, X_{-W}, \eta)  \} 
\end{equation}
contains $\mu_{W}$ with high probability conditional on $W$, $X_{-W}$, and the event $G_{\mu}$:
\begin{equation*}
    P_{\mu}( \mu_{W} \in C_{hyb}(X) | W, X_{-W}, G_{\mu})  = P_{\mu}( X_W - \mu_W < q^{h}_{\frac{1-\alpha}{1-\beta}}(W, X_{-W}, \mu_{W}) |W, X_W, G_{\mu}) = \frac{1-\alpha}{1-\beta}.
\end{equation*}
Based on our earlier discussions, this is sufficient to imply that $C_{hyb}(X)$ from $\eqref{eq:hyb_set}$ will contain $\mu_{W}$ with probability at least $1-\alpha$. 

As was the case for our conditional confidence region \eqref{eq:cond_set}, the hybrid confidence region \eqref{eq:hyb_set} is a little hard to interpret at first. We will get a clearer sense of its benefits when we write it in terms of p-values. As a teaser, the rightmost panel of \Cref{fig:lcb} plots distance between the winner $X_W$ and the hybrid LCB for a $n=10$ dimensional problem. We argue (using our upcoming p-value viewpoint) in \Cref{sec:hybrid_gap_appdx} that, once we fix $\alpha$ and $\beta$, this distance is a function of just the gap $X_W - X_R$ between the winner and runner up and the problem dimension $n$. 

\subsubsection{p-Value Viewpoint} 


Considering data $X \sim N(\mu, I_n)$ with unknown mean $\mu$ and the p-values $p^{\mu_0
}_i = 1 - \Phi(X_i - \mu_0)$, we want to characterize when the hybrid LCB is at least $\mu_0 \in \R$. Examining \eqref{eq:hyb_dist}, \eqref{eq:hyb_quantile}, and \eqref{eq:hyb_set}, we can consider two cases to figure out when this happens. \newline 


\noindent \textbf{Case One - $X_R - \mu_0 \geq z_{1 - \beta_n}$:} If $X_R - \mu_0 \geq z_{1 - \beta_n}$, then $q^h_{\frac{1-\alpha}{1-\beta}}(W, X_{-W}, \mu_0) = -\infty$, so $\mu_0$ cannot be in \eqref{eq:hyb_set}. This case happens precisely when 
\begin{equation*}
    X_R-\mu_0 \geq z_{1 - \beta_n} \iff 1 - \Phi(X_R - \mu_0) \leq 1 - \Phi(z_{1- \beta_n}) \iff p^{\mu_0}_{(2)} \leq \beta_n.
\end{equation*}

\noindent \textbf{Case Two - $X_R - \mu_0 < z_{1 - \beta_n}$:} If $X_R - \mu_0 < z_{1 - \beta_n}$, then $\mu_0$ is not in \eqref{eq:hyb_set} exactly when $X_W - \mu_0$ is at least as large as the $\frac{1-\alpha}{1-\beta}$ quantile $Q$ of a standard normal truncated to be larger than $X_{R} - \mu_0$ but smaller than $z_{1- \beta_n}$. This quantile satisfies 
\begin{equation*}
    \frac{\alpha - \beta}{1- \beta} = \frac{\Phi(z_{1 - \beta_n})  - \Phi(Q) }{\Phi(z_{1-\beta_n})  - \Phi(X_R - \mu_0) } = \frac{1 - \beta_n  - \Phi(Q) }{1 - \beta_n - \Phi(X_R - \mu_0) } 
\end{equation*}
Solving for $Q$ gives 
\begin{equation*}
    Q = \Phi^{-1} \left(1 - \beta_n - \frac{\alpha - \beta}{1 - \beta}\left(1 - \beta_n - \Phi(X_{R} - \mu_0) \right) \right) = \Phi^{-1} \left( \left(1 - \frac{\alpha - \beta}{1 - \beta}\right) (1 - \beta_n) + \frac{\alpha-\beta}{1-\beta}\Phi(X_{R} - \mu_0) \right),
\end{equation*}
so we reject exactly when 
\begin{align*}
        X_{W} - \mu_0 \geq \Phi^{-1} \left( \left(1 - \frac{\alpha - \beta}{1 - \beta}\right) (1- \beta_n) + \frac{\alpha-\beta}{1-\beta}\Phi(X_{R} - \mu_0) \right) \\
        \iff 1 -\Phi(X_{W} - \mu_0) \leq \left(1 - \frac{\alpha - \beta}{1 - \beta}\right)\beta_n + \frac{\alpha-\beta}{1-\beta}(1-\Phi(X_{R} - \mu_0))  \\
        \iff p^{\mu_0}_{(1)} \leq \frac{\alpha-\beta}{1-\beta}p^{\mu_0}_{(2)}  + \left(1 - \frac{\alpha - \beta}{1 - \beta}\right)\beta_n \\
\end{align*}

It turns out we can combine these two cases. Because $p^{\mu_0}_{(1)} \leq p^{\mu_0}_{(2)}$, the fact that $p^{\mu_0}_{(2)} \leq  \beta_n$ in Case One implies that $p^{\mu_0}_{(1)} \leq \beta_n$ also. Therefore in Case One, $p^{\mu_0}_{(1)}$ must be strictly smaller than a mixture of $p^{\mu_0}_{(2)}$ and $\beta_n$:
\begin{equation}
    \label{eq:hybrid_cuttoff_appdx}
    p^{\mu_0}_{(1)} \leq  \frac{\alpha-\beta}{1-\beta}p^{\mu_0}_{(2)}  + \left(1 - \frac{\alpha - \beta}{1 - \beta}\right)\beta_n. 
\end{equation}
Therefore, if we reject according to \eqref{eq:hybrid_cuttoff_appdx}, we will always reject in Case One, and we will reject at the appropriate times in Case Two. 

\subsubsection{Distance Between $X_W$ and Hybrid LCB}
\label{sec:hybrid_gap_appdx}

Consider observing Gaussian data $X \sim N(\mu, I_n)$ and let $W$ and $R$ be the indicies of the winner and runner up respectively. We briefly justify that the distance between $X_W$ and the chybrid LCB for $\mu_W$ depends only on the gap between $X_W - X_R$ and the dimension $n$. Define $D = X_W - \mu_0$ to be distance between $X_W$ and the hybrid LCB $\hat{\mu}$. The hybrid LCB $\hat{\mu}$ satisfies 
\begin{align*}
    & p^{\hat{\mu}}(X_W) = \frac{\alpha-\beta}{1-\beta} p^{\hat{\mu}}(X_R) + \left(1 - \frac{\alpha-\beta}{1-\beta} \right)\beta_n\\
    & 1 - \Phi(X_W - \hat{\mu}) = \frac{\alpha-\beta}{1-\beta} (1 - \Phi(X_R - \hat{\mu})) + \left(1 - \frac{\alpha-\beta}{1-\beta} \right)\beta_n\\ 
    &\iff 1 - \Phi(D) = \frac{\alpha-\beta}{1-\beta} (1 - \Phi(D - (X_W - X_R))) + \left(1 - \frac{\alpha-\beta}{1-\beta} \right)\beta_n.
\end{align*}
Clearly then $D$ is a function of $X_W - X_R$ and $n$.

\subsection{Comparing Hybrid Inference to the Union Bound}
\label{sec:hybrid_sim_appdx}

\begin{figure}
    \centering
    \hspace{-0.035\textwidth}
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{fig/hyb_dist_to_winner_n=10.pdf}
        \caption*{(a) $n=10$}
    \end{minipage}
    \hfill
    \hspace{0.01\textwidth}
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{fig/hyb_dist_to_winner_n=100.pdf}
        \caption*{(b) $n=100$}
    \end{minipage}
    \hfill
    \hspace{0.01\textwidth}
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{fig/hyb_dist_to_winner_n=1000.pdf}
        \caption*{(c) $n=1000$}
    \end{minipage}
    \caption{ For $n=10$ (left), $n=100$ (middle), and $n=1000$ (right) the distance between the hybrid LCB to the winner (dash-dot line) and union bound LCB to the winner (dotted and solid line) with $\alpha=0.05$ and $\beta=0.005$ plotted as a function of the gap between the winning and runner-up observation.}
    \label{fig:hybrid_union}
\end{figure}

As discussed earlier the hybrid cutoff \eqref{eq:hybrid_cutoff_thm} is strictly larger than the union bound cutoff \eqref{eq:union_bound_cutoff} when $p_{(1)} > \beta_n$. Thus, both procedures reject when $p_{(1)} \leq \beta_n$. When $p_{(1)} > \beta_n$, the hybrid procedure rejects but the union bound does not whenever  
\begin{equation*}
    p_{(1)} \in \bigg((\alpha - \beta)p_{(2)},  \frac{(\alpha - \beta)}{1-\beta}p_{(2)} + \left(1 -  \frac{(\alpha - \beta)}{1-\beta}\right)\beta_n \bigg]
\end{equation*}
When $p_{(2)} = 1$ and $n=1$, the length of the interval is $\beta$, which is the largest it can possible be. Thus, in the case where we can have additional rejections, the hybrid cutoff is never more than $\beta$ plus the union bound cutoff. \cite{Andrews2023} suggests taking $\beta=\alpha/10$, so when $\alpha = 0.05$, for example, $\beta=0.005$ is quite small. 

Still, this is not a precise statement about power gain. The computations required to compute the power gain analytically are messy, so instead we gauge the power gain via simulation. We sample data $X \sim N(\mu, I_n)$ for $n=10$ and attempt to reject the winning null $H_W: \mu_W \leq 0$ where $W = \argmin_{i \in [n]} 1 - \Phi(X_i)$ is the index of the smallest p-value $p_i = 1-\Phi(X_i)$.  

We choose $n=10$ because it is a reasonably small dimension size where one may apply hybrid inference (e.g., the main example from \cite{Andrews2023} has $n=13$). Let $R$ denote the index of the runner-up (second smallest p-value). For the dimensions  $n=10, 100, 1000$, \Cref{fig:hybrid_union} 
compares the distance from the winner $X_W$ the hybrid LCB and the union bound LCB. As illustrated in the plot, the benefit of hybrid inference dissapates as the dimension of the problem increases. This is because conditioning on the good event has less and less of an effect as $n$ grows. 

Our simulation results indicate that hybrid inference typically results in a fairly small power gain. We consider two simulated settings: \newline 

\noindent \textbf{Needle in a haystack: } First, we consider a needle in the haystack setting where $\mu_1 > 0$ and all the other $\mu_i$ for $ i\neq 1$ are set to $\mu_2$. We try $\mu_2 = -2, 0, 2$. The power comparison is ploted in \Cref{fig:hybrid_union_power}. Whenever we truly have a needle in the haystack problem, i.e., $\mu_1 > \mu_2$, hybrid inference results in essentially no power gain. The only setting where we see some gain (up to around $0.05$) is when $\mu_2 > \mu_1$. In this setting  we actually have a dense alternative (many small signals). We expect the top two p-values to be close to each other, so conditional methods should perform poorly. The union bound approach indeed performs essentially identically to the level $\beta$ classical test (not pictured). Hybrid, however, manages to eke out some additional power. Both methods pale in comparison to the level $\alpha$ classical test however, which would achieve power $>0.95$ throughout the whole plot (not pictured). For various values of $\sigma_1$ and $\sigma_2$, which we assume are known, we also tried re-running the experiments with  $X_1 \sim N(\mu_1, \sigma_1^2)$ and $X_i \sim N(\mu_2, \sigma_2^2)$ when $i > 1$. The results were not appreciably different.  \newline 

\noindent \textbf{Two possible signals: } Seeing as the hybrid and union bound approaches both reject based on the winning and running-up p-value, we ran a simulation for all pairs $\mu_1, \mu_2 \in \{-3, -2.9, \dots, 2.9, 3 \}$ with $\mu_1 > \mu_2$ and $\mu_1 > 0$. We forcibly set $\mu_i = -\infty$ for $i > 2$. For each setting we ran $N=10000$ to get an empirical estimate of power for each method. Across the $1492$ simulated settings, the median emprical power increase from hybrid was $\approx 0.003$, the $90$th percentile empirical power increase was $\approx 0.023$ and the maximum empirical power increase was $\approx 0.042$. As the results indicate, the power increases from hybrid were negligble for most settings. We also re-ran the same simulations but sampled $X_1 \sim N(\mu, \sigma_1^2)$ and $X_2 \sim N(\mu, \sigma_2^2)$ for various values of $\sigma_1$ and $\sigma_2$, which we assume are known. The results were not appreciably different, and, if anything, the difference in power was notably smaller for some values of $\sigma_1$ and $\sigma_2$.  

\begin{figure}
    \centering
    \hspace{-0.035\textwidth}
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{fig/hybrid_vs_union_null=0.pdf}
        \caption*{(a) $\mu_2 = -2$}
    \end{minipage}
    \hfill
    \hspace{0.01\textwidth}
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{fig/hybrid_vs_union_null=0.pdf}
        \caption*{(b) $\mu_2 = 0$}
    \end{minipage}
    \hfill
    \hspace{0.01\textwidth}
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{fig/hybrid_vs_union_null=2.pdf}
        \caption*{(c) $\mu_2=2$}
    \end{minipage}
    \caption{ For $\mu_2=0$ (left), $\mu_2=-0.5$ (middle), and $\mu_2=-1$ (right) the empirical power over $N=10^4$ trials of the hybrid inference approach versus the union bound approach for the needle in the haystack alternative. One standard error bands are also plotted. For the most part, they are so small that they are hardly visible.  }
    \label{fig:hybrid_union_power}
\end{figure}

\subsection{Conditional inference on winners for exponentials}
\label{sec:exponential_winner_appdx}

Recall the exponential distribution $X \sim \text{Exp}(\lambda_i)$ which has PDF
\begin{equation*}
    f_{\lambda}(x) = 
\begin{cases} 
\lambda e^{-\lambda x} & x > 0, \\ 
0 & x \leq 0 
\end{cases}.
\end{equation*}
Defining $T(x) = 1/x$, we see for $x > 0$ and $\lambda_2 \geq \lambda_1$, the ratio  
\begin{equation*}
   f_{\lambda_2}(x)/f_{\lambda_1}(x) = \frac{\lambda_2}{\lambda_1} \exp\left( - \frac{\lambda_2 - \lambda_1}{T(x)}\right)
\end{equation*}
is monotone non-decreasing in $T(x)$. Thus this family has an MLR in $T(x)$, and the UMP test for $H_0 : \lambda \leq \lambda_0$ thus rejects when $T(X)$ is large, or correspondingly when $X$ is small. In particular, noting that the CDF of $X$ is given by 
\begin{equation*}
F_X(x) = 
\begin{cases} 
1 - e^{-\lambda x} & x > 0, \\ 
0 & x \leq 0 
\end{cases}
\end{equation*}
it rejects according to the p-value $p^{\lambda^0} = 1 - e^{-\lambda_0 X}$ (see \Cref{sec:one_sided_mlr_appdx} for details).

Now consider observing some independent data $X_i \sim \text{Exp}(\lambda_i)$. We know from \Cref{sec:cond_appdx} that the selective p-value $p^{\lambda_0}_{(1)}/p^{\lambda_0}_{(2)}$ is monotone non-decreasing in $\lambda_0$. Using L'hopital's rule, we can compute that 
\begin{align*}
    \lim_{\lambda_0 \downarrow 0} \frac{p^{\lambda_0}_{(1)}}{p^{\lambda_0}_{(2)}} &=  \lim_{\lambda_0 \downarrow 0}  \frac{1 - e^{-\lambda_0 X_{(1)}} }{1 - e^{-\lambda_0 X_{(2)}}}\\
    &= \lim_{\lambda_0 \downarrow 0} \frac{X_{(1)}e^{-\lambda_0 X_{(1)}}}{X_{(2)}e^{-\lambda_0 X_{(2)}}}\\
    &= \frac{X_{(1)}}{X_{(2)}},
\end{align*}
which suffices to imply our claims in the main text. 

\subsection{Number of ties in rank verification}
\label{sec:ties_appdx}

Considering a random vector $X \in \R^n$ let 
\begin{equation}
    Y_i = \frac{X_i - X_j}{2}, \qquad  Y_j = \frac{X_i + X_j}{2}, \qquad  Y_{\ell} = X_{\ell} \text{ for } \ell \neq i, j,
\end{equation}
Suppose that $X_i \geq X_k$ for all $k \neq i$, and $X_i = X_{\ell}$ for some $\ell \neq i$ (i.e., there is at least one tie). We count the number of ties in for the winner in three different cases.

\begin{itemize}
    \item Suppose $Y_j > \max_{k \neq i, j} Y_k$. If any of the $X_k$ for $k \neq i, j$ were equal to $X_i$, then we would have 
    \begin{equation*}
       Y_k = X_k =  \frac{X_i + X_k }{2} \geq \frac{X_i + X_j}{2} = Y_j
    \end{equation*}
    which would be a contradiction. Thus, if there is a tie, the only possible tie is $X_j$. Since, 
    \begin{equation*}
        1 + |\{ \ell \neq i : Y_{\ell} = \max_{k \neq i} Y_k  \}| = 2
    \end{equation*}
    in this case, we are done. 
    \item Suppose that $Y_j < \max_{k \neq i, j} Y_k$. Then some $X_k$ for $k \neq i, j$ must be strictly than larger $X_j$. Otherwise we would have for every $k \neq i, j$ that  
    \begin{equation*}
        Y_j = \frac{X_i + X_j}{2} \geq  \frac{X_i + X_k}{2} \geq X_k = Y_k,
    \end{equation*}
     which is a contradiction. Thus, if there is a tie, the number of ties is one plus the number of $X_k$ for $k \neq i, j$ that are equal to each other. In this case, this matches
    \begin{equation*}
        1 + |\{ \ell \neq i : Y_{\ell} = \max_{k \neq i} Y_k  \}|,
    \end{equation*}
    so we are done.
    \item Now suppose that $Y_j = \max_{k \neq i, j} Y_k$. In this case we must have $X_j = X_i$. If not, then $X_j < X_i$ and  there must be some $\ell \neq i, j$ such that $X_i = X_{\ell}$, so 
    \begin{equation*}
        Y_j =  \frac{X_i + X_j}{2} < \frac{X_i + X_{\ell}}{2} = X_{\ell} = Y_{\ell},
    \end{equation*}
    which is a contradiction. Thus $Y_j = X_i$ in this case, and the number of ties is therefore clearly 
    \begin{equation*}
        1 + |\{ \ell \neq i : Y_{\ell} = \max_{k \neq i} Y_k  \}|.
    \end{equation*}
\end{itemize}

\subsection{p-value Adjustment for Exponential Families}
\label{sec:rank_verficiation_adj_appdx}

Suppose $p$ is a p-value for the null $H_0$ that is selectively dominant given $Z$ and we select $p$ to use for inference according to the selection function 
    \begin{equation*}
        s(x, z) = 
        \begin{cases} 
        1 & \text{if } x < q^+(z), \\
        \frac{1}{N(z)} & \text{if } x \in [q^+(z), q(z)], \\
        0 & \text{otherwise},
        \end{cases},
    \end{equation*}
Then the adjusted p-value \eqref{eq:adjustment} is given by 
\begin{equation*}
    p_{adj} = \frac{\int_0^p s(x, Z) dx }{\int_0^1 s(x, Z)dx} = 
        \begin{cases} 
        \frac{p}{q^{+}(Z) + \frac{1}{N(Z)}(q(Z) - q^{+}(Z)) }  & \text{if } p  < q^+(Z), \\
        \frac{q^{+}(Z)+ \frac{1}{N(Z)}(p - q^{+}(Z))}{q^{+}(Z) + \frac{1}{N(Z)}(q(Z) - q^{+}(Z)) } & \text{if } p \in [q^+(Z), q(Z)], \\
        \end{cases},
\end{equation*}
which can be re-written as 
\begin{equation*}
   p_{adj} = \frac{p - (1-\frac{1}{N(Z)})(p - q^+(Z))_+ }{q^+(Z) + \frac{1}{N(Z)}(q^+(Z) - q(Z))}.
\end{equation*}
This is sufficient to imply the claim from the main text. 


\subsection{Rank verification warm-up additional details}
\label{sec:rank_verification_warm_up_appdx}

\begin{example}[Rank verification in a simple case]
    \label{exm:rank_verification}
    Suppose that $p$ is a selectively dominant p-value for testing the null $H_0$, but we only choose to test $H_0$ when $p < 1/2$. Applying our framework with the p-value $p$ and selection function $s(x) = I_{x < 1/2}$, \Cref{thm:adjustment} tells us that we control selective Type I error if we reject according to the adjusted p-value from \eqref{eq:adjustment} is $p_{adj} = 2p$:
    \begin{equation}
        \label{eq:rank_verification_error_control}
        P_{H_{0}}\left(p_j \leq \frac{\alpha}{2} | S = 1\right) \leq \alpha.
    \end{equation} 
    
    Now, consider data $X_1 \sim N(\mu_1, 1/\sqrt{2})$ and $X_2 \sim N(\mu_2, 1/\sqrt{2})$, the one-sided nulls $H_{0, j}: \mu_j \leq \mu_{-j}$, and their corresponding selectively dominant p-values $p_j = 1 - \Phi(X_j - X_{-j})$ (selective dominance follows from \Cref{exm:mlr}). Denoting the winner $W = \argmax_{j = 1, 2} X_j$, it is now clear rejecting the data-dependent null $H_{0, W}: \mu_W \leq \mu_{-W}$ when $p_{W} \leq \alpha/2$ maintains Type I error control both conditionally on $W$ and marginally. If $H_{0, j}$ is not true, then trivially $P(\text{falsely reject } H_{0, W} \mid W = j) = 0 \leq \alpha$. For the case that $H_{0, j}$ is true, the event $W=j$ is identical to the event $p_j < 1/2$, and hence is the same event as selecting $p_j$ for inference in \eqref{eq:rank_verification_error_control}. Therefore, 
    \begin{align*}
        P(\text{falsely reject } H_{0, W} \mid W = j) &= P\left(p_W \leq \frac{\alpha}{2}  \mid W = j\right)\\
        &= P\left(p_j \leq \frac{\alpha}{2}  \mid W = j \right) \\
        &\leq \alpha,
    \end{align*}
    implying error control conditional on $W$. Marginal error control then follows from the law of total probability:
    \begin{align*}
        P(\text{falsely reject } H_{0, W}) &= \sum_{j=1, 2} P(\text{falsely reject } H_{0, j} \mid W = j)P(W=j) \\
                                          &\leq \alpha \sum_{j = 1, 2} P(W=j)\\
                                          &=\alpha. 
    \end{align*}
    If $\mu_1 = \mu_2$ then the inequalities become equalities and our error control is tight. 
    \end{example}

\subsection{Rank verification additional details}
\label{sec:rank_verification_appdx}

\begin{example}
    \label{exm:rank_verification_exp_fam}
    Suppose $p$ is a p-value for the null $H_0$ that is selectively dominant given $Z$. If we select $p$ to use for inference according to the selection function 
    \begin{equation*}
        s(x, z) = 
        \begin{cases} 
        1 & \text{if } x < q^+(z), \\
        \frac{1}{N(z)} & \text{if } x \in [q^+(z), q(z)], \\
        0 & \text{otherwise},
        \end{cases},
    \end{equation*}
    where $N(z)> 1$ and $0 \leq q^{+}(z) \leq q(z) \leq 1$ are known functions of $z$, then the adjusted p-value from \eqref{eq:adjustment} turns out to be (see \Cref{sec:rank_verficiation_adj_appdx} for computations)
    \begin{equation}
            \label{eq:tie_adj_p_val}
            p_{adj} = f(p, q^+(Z), q(Z), N(Z)) \qquad f(a, b, c, d) =   \frac{a - (1 - \frac{1}{d})(a - b)_+}{b + \frac{1}{d}(c - b)}. 
     \end{equation}
    Therefore, \Cref{thm:adjustment} tells us that rejecting when \eqref{eq:tie_adj_p_val} is at most $\alpha$ is a selective Type I error controlling procedure:
    \begin{equation}
        \label{eq:tie_tool}
        P_{H_0}\left(f(p, q^+(Z), q(Z), N(Z))  \leq \alpha \mid Z, S = 1\right) \leq \alpha.  
    \end{equation} 

    Now, suppose we observe $X$ drawn from the exponential family \eqref{eq:exp_fam} and let $W$ be the index $i \in \mathcal{S}$ of the largest sufficient satistic (with ties broken randomly). For $i \neq j$, \Cref{exm:exp_fam} tells us that the UMPU p-value $p = p_{ij}^{\delta}$ from \eqref{eq:umpu_rank_verification} for the null $H_{0, ij}^{\delta}: \theta_i - \theta_j \leq \delta$ is selectively dominant given the transformed nuisance statistics $Z = \widetilde{T}_{-i}$ from \eqref{eq:reparam}. Taking $q^+(Z) = q^{\delta, +}_{ij}( \widetilde{T}_{-i})$, $q(Z) = q^{\delta}_{ij}( \widetilde{T}_{-i})$, $N(Z) = N_{i}(\widetilde{T}_{-i})$ and $f$ from  \eqref{eq:rank_verification_lower}, \eqref{eq:rank_verification_upper}, \eqref{eq:rank_verification_num_ties}, and \eqref{eq:tie_adj_p_val}, it is now easy to show that rejecting the data-dependent null $H^{\delta}_{0, Wj}$ when $f(p^{\delta}_{Wj}, q^{\delta, +}_{Wj}, q^{\delta}_{Wj}, N_{W}) \leq \alpha$ controls Type I error both conditional on $W$ and marginally. Again it suffices to restrict our attention to indices $i \in \mathcal{I}$, $i \neq j$ that have a positive probability of being the winner (if $i=j$, then we know $H^0_{ii}$ is true and we simply do not reject). If $H^{\delta}_{0, ij}$ is not true, then trivially $P(\text{falsely reject } H^{\delta}_{0, Wj}| W= i) = 0 \leq \alpha$. For the case that $H^{\delta}_{0, Wj}$ is true (and $i \neq j$), the event $W=i$ is the same event as selecting $p_{0, ij}^{\delta}$ for inference in \eqref{eq:tie_adj_p_val}, so 
    \begin{align*}
        P(\text{falsely reject } H^{\delta}_{0, Wj}| W= i) &= P(f(p^{\delta}_{Wj}, q^{\delta, +}_{Wj}, q^{\delta}_{Wj}, N_{Wj}) \leq \alpha | W= i) \\
        &= P(f(p^{\delta}_{ij}, q^{\delta, +}_{ij}, q^{\delta}_{ij}, N_i) \leq \alpha | W = i) \\
        &\leq \alpha. 
    \end{align*}
    Marginal error control follows from the usual law of total probability argument.

    We can reject the data-dependent global null when  $\cap_{j \in \mathcal{S} - W} H^{\delta}_{0, Wj}$ when we reject all of the individual nulls $H^{\delta}_{0, Wj}$ for $j \in \mathcal{S} - W$. It is straightforward to see that this will control Type I error both conditionally on $W$ and marginally: If there is an $i \in \mathcal{I}$ for which $\cap_{j \in \mathcal{S} - i} H^{\delta}_{0, ij}$ is false, then trivially $P(\text{falsely reject } \cap_{j \in \mathcal{S} - W} H^{\delta}_{0, Wj}| W= i) = 0 \leq \alpha$ for this $i$. Otherwise, there exists a $k \in \mathcal{S} - i$ for which $\theta_i \leq \theta_k$, and 
    \begin{align*}
        P(\text{falsely reject } \cap_{j \in \mathcal{S} - W} H^{\delta}_{0, Wj}| W= i) \leq P(\text{falsely reject } H^{\delta}_{0, Wk}| W= i) \leq \alpha.\
    \end{align*} 
    Again, marginal error control follows from the usual law of total probability argument.
\end{example}

\subsection{Inference on Winners in Exponential Families}

We can use the same tools from the previous sub-section  to do inference on the winner in multiparameter exponential families. Again consider data $X$ drawn from the exponential family \eqref{eq:exp_fam} and let the winner $W$ be the index $i \in \mathcal{S} \subseteq [p]$ corresponding to the largest sufficient statistic $T_i$, with ties broken randomly.

The p-value 
\begin{equation}
    \label{eq:ump_exp_fam}
    p_i^{\theta_0}(T(X)) = G^{\theta_0, +}_i(T_i|T_{-i}) + U_{i, aux}(G^{\theta_0}_i(T_i|T_{-i}) - G^{\theta_0, +}_i(T_i|T_{-i})),
\end{equation}
corresponds to the UMP test for $H_{0, i}^{\theta_0}: \theta_i \leq \theta_0$, where, like before,
\begin{equation*}
    G^{\theta_0}_i(t_i | t_{-i}) = P_{\theta_0}(T_i \geq t | T_{-i} = t_{-i}) \qquad G^{\theta_0, +}_i(t) = \lim_{u \downarrow t}(T_i \geq t | T_{-i} = t_{-i} ),
\end{equation*}
denote the conditional left-continuous survival function of $T_i$ and its right-hand limit, and $U_{i, aux}$ are $\text{Unif}([0,1])$ random variables that are independent from each other and $X$.

To come up with a test for the data dependent null $H_{0, W}^{\theta_0}$ (which we will then invert to get an LCB for the winning parameter), we need to apply a similar adjustment to our p-value as in \Cref{exm:rank_verification_exp_fam}. Rather than re-work through essentially identical arguments, we simply define the analogous quantities to those from the previous sub-section,
\begin{equation}
    \label{eq:exp_fam_lower}
    q^{\theta_0, +}_i(T_{-i}) = G^{\theta_0, +}_i(\max_{k \in \mathcal{S} - i } T_k | T_{-i}), 
\end{equation} 
\begin{equation}
    \label{eq:exp_fam_upper}
    q^{\theta_0}_i(T_{-i}) = G^{\theta_0}_i(\max_{k \in \mathcal{S} - i } T_k | T_{-i}),
\end{equation} 
\begin{equation}
    \label{eq:exp_fam_num_ties}
    N_i(T_{-i}) = 1 + | \{k \neq i: T_k = \max_{\ell \in \mathcal{S} - i} T_\ell \}|,
\end{equation}
and state the final result in \Cref{cor:cond_exp_fam}. In \Cref{coming_soon} we use a modification of the arguments in \cite{Fithian2017} to show that our test $H^{\theta_0}_{0, W}$ is selectively UMPU, meaning it is UMPU amongst all tests that are are valid conditional on $W$. Hence, as stated in \Cref{cor:cond_exp_fam}, its inversion is selectively uniformly most accurate unbaised (UMAU) in the analogous sense. 

\begin{corollary}[Conditional inference for multiparameter exponential families ]
    \label{cor:cond_exp_fam}
    Let $X$ be drawn from the exponential family \eqref{eq:exp_fam} and let $W$ be the index $i \in \mathcal{S}$ corresponding to the largest sufficient statistic $T_i$ (with ties broken randomly). If $p^{\theta_0}_i$ is the UMP p-value \eqref{eq:ump_exp_fam} for testing $H^{\theta_0}_{0, i} : \theta_i \leq \theta_0 $ and $q^{\theta_0, +}_i$, $q^{\theta_0}_i$, and $N_i$ are as in \eqref{eq:exp_fam_lower}, \eqref{eq:exp_fam_upper}, and \eqref{eq:exp_fam_num_ties}, then 
    \begin{equation}
    \label{eq:cond_lcb_exp_fam}
    \left\{\theta_0 :  \frac{p^{\theta_0}_W - \left(1 - \frac{1}{N_i} \right)(p^{\theta_0}_W - q^{\theta_0, +}_W)_+ }{q^{\theta_0, +}_W + \frac{1}{N_i}(q^{\theta_0}_W - q^{\theta_0, +}_W) } > \alpha \right\} 
    \end{equation}
    is an LCB for $\theta_W$ that holds with probability exactly $1-\alpha$ conditional on $W$ (and therefore also marginally), and it is selectively UMAU conditional on $W$.
\end{corollary}

Briefly, we point out that selecting the largest sufficient statistic does not always correspond to selecting the ``most promising'' effect. Even though $T_i > T_j$, it might be the case that $p^{\theta_0}_{j}$ is smaller than $p^{\theta_0}_{i}$.  Unlike in \Cref{sec:winner}, we cannot simply have the winner correspond to the smallest p-value for two reasons. First, in this more general setting, the index resulting in the smallest p-value changes depending on $\theta_0$ (see \Cref{sec:small_p_val_appdx} for an example). Hence, it is not well-defined to determine the winner by looking at the smallest p-value (and we thus have to deal with ties). Second, even if we settled on a $\theta_0$ to prioritize, because the p-values have intricate correlation strucutre, the selection event $p^{\theta_0}_{i} < \max_{j \neq i} p^{\theta_0}_{j}$ corresponds to a very complicated selection function that we would have to try and characterize on a case-by-case basis. Indeed, this discussion applies to the rank verification problem for exponential families we discussed previously as well. 

\iffalse

\subsection{Smallest p-value and the Winner}
\label{sec:small_p_val_appdx}

We give a quick example showing that, in parameteric settings, we cannot always use the smallest p-value to determine the winner. Suppose that $X_1 \sim N(\mu_1, \sigma_1^2)$ and $X_2 \sim N(\mu_2, \sigma_2^2)$ with the $\mu_i$ unknown but $\sigma_i$ known. The UMP p-value for testing the null $H_{0, i}^{\mu_0}: \mu_i \leq \mu_0$ is given by $p^{\mu_0}_i = 1- \Phi(\frac{X_i - \mu_0}{\sigma})$. Suppose that $\sigma_1 = 1$, $\sigma_2 = 3$ and we observe $X_1 = 1$ and $X_2=2$. Then
\begin{equation*}
    p^0_1 = 1 - \Phi(1) < 1 - \Phi(2/3) = p^0_2
\end{equation*}
but 
\begin{equation*}
    p^1_1 = 1 - \Phi(0) > 1 - \Phi(1/3) = p^0_2.
\end{equation*}

\subsection{Comparing Selection Functions}
\label{sec:publication_bias_appdx}

Consider a selectively dominant p-value $p$ for the null $H_0$ that is valid conditional on $Z$, and two binary selection variables $S_1 \in {0, 1}$ and $S_2 \in {0, 1}$ whose joint relationships with $p$ are governed by two different selection functions:
\begin{equation*}
    s_1(x, z) = P(S_1 = 1 | p=x, Z=z),
\end{equation*} 
\begin{equation*}
    s_2(x, z) = P(S_2 = 1 | p=x, Z=z)
\end{equation*} 
Sometimes, if the selection function $s_1(x, z)$ is harsher than the other $s_2(x, z)$ then using \Cref{thm:adjustment}'s adjusted p-value under $s_1(x, z)$ maintains error control even when $p$ is selected according to $s_2(x, z)$. In particular, whenever
\begin{equation*}
    p_{adj, 2} =  \frac{\int_0^p s_2(x, Z) dx }{\int_0^1 s_2(x, Z) dx}  \leq \frac{\int_0^p s_2(x, Z) dx }{\int_0^1 s_2(x, Z) dx} = p_{adj, 1}
\end{equation*}
we have from \Cref{thm:adjustment} that 
\begin{equation*}
    P_{H_0}(p_{adj, 1}  \leq \alpha \mid S_2 = 1 ) \leq   P_{H_0}(p_{adj, 2}  \leq \alpha \mid S_2 = 1 )  \leq \alpha.
\end{equation*}

One example that is relevant to our publication bias application is where $s_1(x) = I_{x \leq \alpha}$ and $s_2(x)$ is any function that takes some constant value $c \in [0, 1]$ (we make no assumptions about what values $s_2(x)$ takes outside of $(0, \alpha]$). The adjusted p-value $p_{adj, 1} = p/\alpha$ remains unchanged if we take $s_1(x) = cI_{x \leq \alpha}$, so instead (scaling the selection function by a positive constant does not change the adjustment), so intstead imagine working with this selection function. Then we see that 

\begin{align*}
    \frac{\int_0^p s_2(x, Z) dx }{\int_0^1 s_2(x, Z) dx}  \leq \frac{\int_0^p s_1(x, Z) dx }{\int_0^1 s_1(x, Z) dx} &\iff  \frac{\int_0^1 s_2(x, Z) dx}  {\int_0^p s_2(x, Z) dx }\geq \frac{\int_0^1 s_1(x, Z) dx}{\int_0^p s_1(x, Z) dx }\\
    &\iff 1 + \frac{\int_p^1 s_2(x, Z) dx}  {\int_0^p s_2(x, Z) dx }\geq 1 + \frac{\int_p^1 s_1(x, Z) dx}{\int_0^p s_1(x, Z) dx }\\
    &\iff \frac{\int_p^1 s_2(x, Z) dx}  {\int_0^p s_2(x, Z) dx }\geq \frac{\int_p^1 s_1(x, Z) dx}{\int_0^p s_1(x, Z) dx }
\end{align*}
Examine the last inequality. When $p > \alpha$ the numerator of the right-hand side of is $0$, so it must be true. If $p \leq \alpha$, then the denominators of both sides of the last inequality are equal but the numerator of the left-hand side is at least that of the numerator of the right-hand side, which is at most $c(\alpha - p)$. Therefore, the entire string of inequalities, including the first one, is true, implying that $p_{adj, 2} \leq p_{adj, 1}$. 

Now we briefly consider the case that $c=0$, so $s_2(x)$ is $0$ on the interval $[0, \alpha]$. It is clear for $p$ such that $\int_0^p s_2(x)dx = 0$ that the first inequality is true (the remaining inequalities are no longer well defined since they require dividing by zero). If $p$ is such that  $\int_0^p s_2(x)dx > 0$, then we must have $p > \alpha$, so again the last inequality is true by the same reasoning as above (now the remaining inequalities are well defined). Again, we find that $p_{adj, 2} \leq p_{adj, 1}$.
 

\subsection{Normal with different variances}

Imagine observing $n$-dimensional data $X \sim N(\mu, \Sigma)$ where $\Sigma$ is a diagonal matrix that is known. We consider testing the nulls $H_{, i}^{\mu_0}: \mu_i \leq \mu_0 $ using the p-values $p^{\mu_0}_i = 1 - \Phi((X_i - \mu_0)/\sqrt{\Sigma_{ii}})$. 

We want to test according to the most promising p-value for testing the null $H_{0, i}^{0}: \mu_i \leq 0 $. That is, we define the winning index $W = \argmax_{i \in [n]} X_i/ \sqrt{\Sigma_{ii}}$ and want to do inference on the winning mean $\mu_W$. Define $q^{\mu_0}_{jk} = 1 - \Phi(X_k/\sqrt{\Sigma_{kk}} - \mu_0/\sqrt{\Sigma_{ii}} )$, so that $W = j$ exactly when $p^{\mu_0}_j \leq q^{\mu_0}_{jk}$ 



We cover two ways of doing so. 

\textbf{Conditional Inference: } In the conditional inference case, we select $p^{\mu_0}_j$ to use for inference exactly when it is less than $\max_{k \neq j} q^{\mu_0}_{jk}$. Thus, 
\begin{equation*}
    \{ \mu_0 :  p^{\mu_0}_W > \alpha \max_{k \neq W} q^{\mu_0}_{Wk} \}
\end{equation*}
gives a CLB for $\mu_W$ that has exact $1-\alpha$ coverage conditional on $W$. \newline

\textbf{Hybrid Inference: }
\begin{equation*}
    \left\{ \mu_0 :  p^{\mu_0}_W > \frac{\alpha - \beta}{1-\beta} \max_{k \neq W} q^{\mu_0}_{Wk} + \left(1 - \frac{\alpha-\beta}{1-\beta} \right) \beta_n \right\}
\end{equation*}


Now we consider the case of data splitting, so we have data $X^1_{i}, X^2_{i} \overset{i.i.d}{\sim} N(\mu_i, 2\Sigma_{ii})$. We select the winner according to the first split, so $\widetilde{W} = \argmax_{i \in [n]} X^1_{i}/ \sqrt{\Sigma_{ii}}$. Define the split p-values $p^{\mu_0, \ell}_i = 1 - \Phi((X^{\ell}_{i} - \mu_0)/\sqrt{2 \Sigma_{ii}} ) $. Like earlier, define the split thresholds $q^{\mu_0, \ell}_{jk} = 1 - \Phi(X^{\ell}_k/\sqrt{\Sigma_{kk}} - \mu_0/\sqrt{\Sigma_{ii}} )$ \newline 


\textbf{Data Splitting: } In data splitting we select according to the first p-value and the do inference using the second:

\begin{equation*}
    \{\mu_0 : p^{\mu_0, 2}_{\widetilde{W}} > \alpha \}
\end{equation*}

This gives an LCB for $\mu_0$ that holds conditionally on $\tilde{W}$ with probability exactly $1-\alpha$. 



\textbf{Data Carving: } In data carving we select according to the first p-value but still do inference by combining both p-values:

\begin{equation*}
    \left\{\mu_0 : -2  \left( \log\left( \frac{p^{\mu_0, 1}_{\widetilde{W}}}{\max_{k \neq \tilde{W}} q^{\mu_0, 2}_{\widetilde{W}k}} \right) + \log\left( p^{\mu_0, 2}_{\widetilde{W}} \right)\right) > \alpha \right\}
\end{equation*}

\textbf{Data Carving: } In most powerful data carving we still use the joint p-value 
\begin{equation*}
    p_i = 1 - \Phi\left( \frac{ \frac{X_i^1 + X_i^2}{2} - \mu_0}{\sqrt{\Sigma_{ii}}} \right) 
\end{equation*}

We select because $X^1_i > \sqrt{\frac{\Sigma_{ii}}{\Sigma_{jj}}} X^1_j$. So the selection probability is 
\begin{align*}
    s(x) &= P( X^1_i > t | p_i = x ) \\
         &=  P( X^1_i > t | \frac{X^1_i + X^2_i}{2} =  \sqrt{\Sigma_{ii}}\Phi^{-1}(1 -x) + \mu_0 )\\
         &= 1 - \Phi((t - \mu_0)/\sqrt{\Sigma_{ii}} - \Phi^{-1}(1 - x))
\end{align*}

Where we have noted that $X_i^1 |  (X_i^1 + X_i^2)/2 = y \sim N(y, \Sigma_{ii})$ 
\fi 

\subsection{Post selection inference for the LASSO}
\label{sec:lasso_appdx}

Coming soon. 

\subsection{Data carving for Gaussian file-drawer}
\label{sec:carve_appdx}

We have two data samples $X_1 \sim N(\mu, 2)$ and $X_2 \sim N(\mu, 2)$ that are independent and want to test $H_0 : \mu \leq 0$. Suppose we only do inference because we observed that $X_1  > t$ for some threshold $t$. If we consider the p-value $p_{full} = 1 - \Phi((X_1 + X_2)/2  )$, then our selection function is given by 

\begin{align*}
    s(x) &= P( X_1 > t | p_{full} = x ) \\
         &=  P( X_1 > t | \frac{X_1 + X_2}{2} =  \Phi^{-1}(1 -x) )\\
         &= 1 - \Phi(t - \Phi^{-1}(1 - x))
\end{align*}
where we have used that 
\begin{equation*}
    \begin{bmatrix}
    X_1 \\ \frac{X_1 + X_2}{2}
    \end{bmatrix} \sim N \left(\begin{bmatrix}
        \mu \\ \mu
        \end{bmatrix}, \begin{bmatrix}
            2  & 1 \\ 1 & 1
            \end{bmatrix} \right)
\end{equation*}
so 
\begin{equation*}
    X_1 | \frac{X_1 + X_2}{2} = y \sim N(y, 1)
\end{equation*}
Thus our corrected p-value is given by 
\begin{equation*}
    p_{carve} = \frac{\int_0^{p_{full}}  1 - \Phi(t - \Phi^{-1}(1-x) )   dx }{\int_0^1 1 - \Phi(t - \Phi^{-1}(1-x) ) dx}
\end{equation*}
\begin{equation*}
    p_{carve} = \frac{\int_{\bar{X}}^{\infty} \phi(z) (1 - \Phi(t - z))   dz }{\int_{-\infty}^{\infty} \phi(z) (1 - \Phi(t - z))  dz } = \frac{\int_{\bar{X}}^{\infty} \phi(z) (1 - \Phi(t - z))   dz }{1 - \Phi(t/\sqrt{2})   }
\end{equation*}

We now show that $p_{carve}$ is monotone non-decreasing in $t$. Letting $Z$ and $Y$ be independent standard normal random variables and fixing some constant $a$, the selective p-value is given by 
\begin{equation*}
   p_{carve} = \frac{P(Z + Y > t, Z > a)}{P(Z + Y > t)} = P(Z > a | Z + Y > t)
\end{equation*}
for $a = \bar{X}$. Letting $W = Z + Y$ we can write $Z = \frac{1}{2}W +\epsilon$ where $\epsilon$ is independent of $W$. This gives us 
\begin{equation*}
    p_{carve} = P(\frac{1}{2} W + \epsilon > a | W > t) = E[P(W > 2(a - \epsilon)| W > t, \epsilon)| W > t ] = E[P(W > 2(a - \epsilon)| W > t, \epsilon)]
\end{equation*}
Then the fact that $p_{carve}$ is monotone non-decreasing in $t$ follows from the fact that $P(W > c | W > t)$ is monotone non-decreasing in $t$ for every constant $c$:
\begin{equation*}
{P(W > c |W >t)} = \begin{cases} 
\frac{P(W > c)}{P(W > t)} & \text{if } t \leq c, \\
1 & \text{if } t > c.
\end{cases}
\end{equation*}

\section{Selective Dominance and One-Sided Testing}
\label{sec:one_sided_appdx}

In this appendix, we establish the selective dominance property for UMP p-values in MLR familes and UMPU p-values in exponential families. We also show that in these cases, the adjusted p-value from \Cref{thm:adjustment} is monotone in the parameter, under suitable conditions. Throughout the appendix, we draw from the discussion and proof strategy in Appendix B.1 of \cite{Lei}.

\subsection{MLR Families}
\label{sec:one_sided_mlr_appdx}

We consider a parametric family $P_{\theta}$ parameterized by a real parameter $\theta \in R$ such that each $P_{\theta}$ has density $p_{\theta}(x)$ with respect to some carrier measure $\mu$. We will suppose that these densities share support (i.e., for any two $\theta$ and $\theta'$ we have $p_{\theta}(x) > 0 \iff p_{\theta'}(x)> 0$). Further, we suppose that for any $\theta < \theta'$, the likelihood ratio $p_{\theta'}(x)/p_{\theta}(x)$ is a monotone non-decreasing function of some real-valued function $T(x)$ on this support (i.e, for any $x_1 \leq x_2$ with $p_{\theta}(x_1), p_{\theta}(x_2) > 0$, we have $p_{\theta'}(x_1)/p_{\theta}(x_1) \leq p_{\theta'}(x_2)/p_{\theta}(x_2)$). 

Recall that for a testing problem, the critical function $\phi(x)$ (see \cite[Section 3.1]{Lehmann}) tells us the probability of rejecting the null having observed data $x$, so $\phi(X) = P(\text{reject } H_0 | X)$. From Theorem 3.4.1 of \cite{Lehmann} we know the test governed by the critical function 
\begin{equation}
    \label{eq:mlr_test}
    \phi(x) = \begin{cases}
        1 &\text{if } T(x) > C  \\
        \gamma &\text{if } T(x) = C  \\
        0 & \text{otherwise }
    \end{cases}
\end{equation}
is UMP for testing $H_0 : \theta \leq \theta_0$ against the alternatives $H_a : \theta > \theta_0 $ so long $C$ and $\gamma$ satisfy
\begin{equation}
    \label{eq:constraint}
    P_{\theta_0}(T(X) > C) + \gamma P_{\theta_0}(T(X) = C) = \alpha.
\end{equation}
Denote the left-continuous survival function of $T(X)$ and its righthand limit under $P_{\theta_0}$ as
\begin{equation*}
    G(t) = P_{\theta_0}(T(X) \geq t) \qquad G^+(t) = \lim_{u \downarrow t} G(u).
\end{equation*}
Since $G(t)$ is a monotone non-increasing function, we can also define its generalized inverse 

\begin{equation*}
    G^{-1}(z) = \inf \{ t : G(t) \leq z\},
\end{equation*}

\Cref{lem:setting_constants} gives a natural way to set $C$ and $\gamma$ in \Cref{eq:mlr_test} to get an UMP test. 

\begin{lemma}[An UMP test]
    \label{lem:setting_constants} Adopting the convention that $0/0 = 0$, taking $C = G^{-1}(\alpha)$ and $\gamma = (\alpha - G^+(C))/(G(C) - G^+(C))$
    in \Cref{eq:mlr_test} gives an UMP test.
\end{lemma}

\begin{proof}
    At continuity points $t$ of $G(\cdot)$, we have $G(G^{-1}(t)) = t$ and also $P(T(X) = t) = 0$. Thus, if $C = G^{-1}(\alpha)$ is a continuity point of $G(\cdot)$, then $G^+(C) = P_{\theta_0}(T(X) > C) = P_{\theta_0}(T(X) \geq C) = G(C) = \alpha$ and $\gamma=0$, so the constraint \eqref{eq:constraint} is immediately satisfied. 

    If $C = G^{-1}(\alpha)$ is not a continuity point of $G(\cdot)$, then $G(C) - G^+(C) > 0$ and the constraint \eqref{eq:constraint} is also immediately satisfied. To ensure that we still have a valid test, however, we need $\gamma \in [0, 1]$. This is true so long as $\alpha \in [G^{+}(C), G(C)]$. We know that $G(t) \leq \alpha$ for any $t > C$, so $G^{+}(C) \leq \alpha$. If $G(C) < \alpha$, then we could find some $t^{-} < C$ such that $G(C) < \alpha$ by left-continuity, but this would contradict that $C = G^{-1}(\alpha)$, finishing the proof. 
\end{proof}

Letting $U_{aux} \sim \text{Unif}([0, 1])$ be auxiliary randomness that is independent of $X$, a simple way to instantiate the test from \Cref{lem:setting_constants} is to reject whenever $T(X) > C$ or when $T(X) = C$ and $U_{aux} \leq \gamma$. \Cref{lem:fuzzy} explains how this is the same as rejecting when the p-value, termed as a fuzzy p-value in \cite{Geyer}, 
\begin{equation}
    \label{eq:mlr_p_val}
    p = G^+(T(X)) + U_{aux}(G(T(X)) - G^+(T(X))) 
\end{equation}
is at most $\alpha$. 

\begin{lemma}[Fuzzy p-value is UMP]
    \label{lem:fuzzy}
    Rejecting $H_0: \theta \leq \theta_0$ when the fuzzy p-value \eqref{eq:mlr_p_val} is at most $\alpha$ instantiates the test from \Cref{lem:setting_constants}, and is therefore UMP. 
\end{lemma}

\begin{proof} We rewrite 
    \begin{equation*}
        p = (1 - U_{aux})G^+(T(X)) + U_{aux} G(T(X))
    \end{equation*}
    and consider four  cases. 
    \begin{itemize}
        \item If $t < G^{-1}(\alpha)$ then we can find some $t^+ > t$ such that $G(t^+) > \alpha$. Thus $G(t) \geq G^{+}(t) > \alpha$. So $p > \alpha$ whenever $T(X) < G^{-1}(\alpha)$ 
        \item If $t = G^{-1}(\alpha)$ and $G^{-1}(\alpha)$ is a continuity point of $G(\cdot)$, then $G^{+}(t) = G(t) = \alpha$. Thus, in this case $p \leq \alpha$ whenever $T(X) = G^{-1}(\alpha)$ and $U_{aux} \leq  \frac{\alpha - G^+(G^{-1}(\alpha))}{G(G^{-1}(\alpha)) - G^+(G^{-1}(\alpha))} = \infty$. 
        \item If $t = G^{-1}(\alpha)$ and $G^{-1}(\alpha)$ is not a continuity point of $G(\cdot)$, then we must have that $G(t) - G^+(t)  > 0$. Also by right continuity we have $G(t) \geq \alpha$ and by how $G^{-1}(\cdot)$ is defined we have $G^+(t) \leq \alpha$. In this case $p \leq \alpha$ also whenever $T(X) = G^{-1}(\alpha)$ and $U_{aux} \leq  \frac{\alpha - G^+(G^{-1}(\alpha))}{G(G^{-1}(\alpha)) - G^+(G^{-1}(\alpha))}$. 
        \item If $t > G^{-1}(\alpha)$ then $G^+(t) \leq G(t) \leq \alpha$. So $p \leq \alpha$ whenever $T(X) > G^{-1}(\alpha)$. 
    \end{itemize}


    This implies the following set equalities:

    \begin{align*}
        \{p \leq \alpha \} &= \{T(X) > G^{-1}(\alpha) \} \cup \left\{T(X) =  G^{-1}(\alpha), U_{aux} \leq \frac{\alpha - G^+(G^{-1}(\alpha))}{G(G^{-1}(\alpha)) - G^+(G^{-1}(\alpha))} \right\}\\
                           &= \{T(X) > C \} \cup \left\{T(X) =  C, U_{aux} \leq \gamma \right\}\\
    \end{align*}

\end{proof}


\Cref{lem:uniform} shows that $p \sim \text{Unif}([0, 1])$ under $P_{\theta_0}$, which will be useful for us later. 

\begin{lemma}[Fuzzy p-value is uniform at null boundary]
    \label{lem:uniform}
    Under $P_{\theta_0}$, the p-value \eqref{eq:mlr_p_val} has a $\text{Unif}([0, 1])$ distribution.
\end{lemma}

\begin{proof}

Using the set equality from \Cref{lem:fuzzy} but replacing $\alpha$ with $z \in (0, 1)$, we find

\begin{align*}
        &P_{\theta_0}(G^+(T(X)) + U_{aux}(G(T(X)) - G^+(T(X))) \leq z)\\
        &= P_{\theta_0}(T(X) > G^{-1}(z)) + P_{\theta_0}\left(T(X) = G^{-1}(z),  U \leq \frac{z - G^+(G^{-1}(z))}{G(G^{-1}(z)) - G^{+}(G^{-1}(z))}\right)\\
        &= P_{\theta_0}(T(X) > G^{-1}(z)) + P_{\theta_0}(T(X) = G^{-1}(z)) P_{\theta_0}\left(U \leq \frac{z - G^+(G^{-1}(z))}{G(G^{-1}(z)) - G^{+}(G^{-1}(z))}\right)\\
        &=G^+(G^{-1}(z)) + (G(G^{-1}(z)) - G^{+}(G^{-1}(z))) \cdot \frac{z - G^+(G^{-1}(z))}{G(G^{-1}(z)) - G^{+}(G^{-1}(z))}\\
        &= z.
    \end{align*}

\end{proof}

Now we can show that $p$ is a selectively dominant p-value for testing the null $H_0: \theta \leq \theta_0$. In what follows, we consider some fixed $\theta \leq \theta_0$ and prove some facts that allow us to relate the distribution of $T(X)$ under $P_{\theta_0}$ to its distribution under $P_{\theta}$.

\begin{lemma}[Distribution of $T(X)$]
    \label{lem:stan_machine}
    Let $g_{\theta}(T(x))$ be a non-increasing function that equals the likelihood ratio $p_{\theta}(x)/p_{\theta_0}(x)$ on the support and 
    \begin{equation*}
        \nu(A) = \int I(T(x) \in A) p_{\theta_0}(x) \mu(dx)
    \end{equation*}
    be the meausre of $T(X)$ under $X \sim P_{\theta_0}$. Then 
    \begin{equation*}
        P_{\theta}(T(X) \in A) = \int_A g_{\theta}(t) \nu(dt)
    \end{equation*}
\end{lemma}

\begin{proof}
We know that 
\begin{align*}
    P_{\theta}(T(X) \in A) = \int I(T(x) \in A) p_{\theta_0}(x)g_{\theta}(T(x)) \mu(dx), 
\end{align*}
so we need to show that 
\begin{equation}
    \label{eq:stan_machine_to_show}
    \int I(T(x) \in A) p_{\theta_0}(x) g_{\theta}(T(x)) \mu(dx) =  \int_A g_{\theta}(t) \nu(dt)
\end{equation}
If $g_{\theta}(T(x)) = I(T(x) \in A')$ happens to be an indicator then \eqref{eq:stan_machine_to_show} holds. Therefore, we can apply the standard machine (see the discussion after Equation 42 in \cite{Lei}) to show that \eqref{eq:stan_machine_to_show} holds for all non-negative functions $g_{\theta}(\cdot)$.  
\end{proof}


\begin{lemma}[Distribution of $(T(X), U_{aux})$]
    \label{lem:pi_lambda}
    If $\omega$ denotes the product measure of $\nu$ and Lesbesgue measure $\lambda$ on $[0, 1]$, i.e., the distribution of $(T(X), U_{aux})$ under $P_{\theta_0}$, then 
    \begin{equation}
    \label{eq:joint_dist}
        P_{\theta}((T(X), U) \in B) = \int_{B} g_{\theta}(t) \omega(dt, du)
    \end{equation}
\end{lemma}

\begin{proof}
    We will first argue that \eqref{eq:joint_dist} holds for any $B$ which is a product set $A_1 \times A_2$. We can futher reduce to the case that $g_{\theta}(T(x)) = I(T(x) \in A_1')$ is an indicator. Then we see using our previous lemma that 
    \begin{align*}
        P_{\theta}((T(X), U_{aux}) \in A_1 \times A_2) &= P_{\theta}(T(X) \in A_1)P(U_{aux} \in A_2)\\
                                          &= \int_{A_1} g_{\theta}(t) \nu(dt) \cdot \int_{A_2} \lambda(du)\\
                                          &= \int_{A_1 \cap A_1'} \nu(dt) \cdot \int_{A_2} \lambda(du)\\
                                          &= \int_{A_1 \cap A_1' \times A_2} \omega(dt, du)\\
                                          &= \int_{A_1 \times A_2} g_{\theta}(t)\omega(dt, du)
    \end{align*}
    To handle the case of general $g_{\theta}(\cdot)$ we can again simply apply the standard machine. 
    
    The full result then follows from an application of the $\pi-\lambda$ theorem: the set of $B$ for which \eqref{eq:joint_dist} holds is a $\lambda$-system, and \eqref{eq:joint_dist} holds for every set in the $\pi$ system of all product sets $B = A_1 \times A_2$. 
\end{proof}

Note that our p-value $p$ is a deterministic function of $T(X)$ and $U_{aux}$:
\begin{equation*}
    p = m(T(X), U_{aux}) \qquad m(t, u) = G^+(t) + u(G(t) - G^+(t)).
\end{equation*}
As such, we sometimes write our selection function as a function of $T(X)$ and $U_{aux}$:
\begin{equation*}
    s(t, u) = s(m(t, u)). 
\end{equation*}
We use this abuse of notation in our next lemma, which characterizes the conditional distribution of $T(X)$ given selection.

\begin{lemma}[Distribution of $(T(X), U_{aux})$ given selection]
    For any selection function $s(x)$ under which $p$ has a positive probability of selection under $P_{\theta}$, 
    \begin{equation*}
        P_{\theta}((T(X), U_{aux}) \in B| S = 1 ) = \frac{\int_{B} g_{\theta}(t)s(t, u) \omega(dt, du)}{ \int g_{\theta}(t) s(t, u) \omega(dt, du) }
    \end{equation*}
\end{lemma}

\begin{proof} First note that
    \begin{equation*}
        P_{\theta}( (T(X), U_{aux}) \in B | S= 1 ) = \frac{P_{\theta}((T(X), U_{aux}) \in B, S = 1) }{P_{\theta}(S = 1)}.
    \end{equation*} 
    Thus it suffices to show for any set $B$ that 
    \begin{equation*}
        P_{\theta}( (T(X), U_{aux}) \in B,  S= 1 ) = \int_B g_{\theta}(t) s(t, u) \omega(dt, du). 
    \end{equation*} 
    By the definition of conditional expectation
    \begin{align*}
        P_{\theta}( (T(X), U_{aux}) \in B,  S= 1 ) &= E_{\theta}[E_{\theta}[ I(S=1) \mid  T(X), U_{aux}] I((T(X), U_{aux}) \in B) ]\\
                                                   &= E_{\theta}[ s(T(X), U_{aux}) I((T(X), U_{aux}) \in B) ]
    \end{align*}
    If $s(t, u) = I_{(t, u) \in B}$ is an indicator function, then the result is implied by our previous lemma. We again get the result for general selection functions $s(t, u)$ by applying the standard machine. 
\end{proof}


With these lemmas under our belt, we can show \Cref{prop:mlr_sel_dom}, the main result of this sub-section. Since $p \sim_{P_{\theta_0}} \text{Unif}([0, 1])$ by \Cref{lem:uniform}, this proposition is sufficient to imply selective dominance. 

\begin{proposition}
    \label{prop:mlr_sel_dom}
    For any selection function $s(x)$ for which $p$ has positive probability of selection under both $\theta$ and $\theta_0$,  
    \begin{equation*}
        P_{\theta}(p \leq z | S = 1)  \leq P_{\theta_0}(p \leq z | S = 1).
    \end{equation*}
\end{proposition}

\begin{proof}

Fix $z \in (0, 1)$. If $z$ is such that $P_{\theta}(p \leq z | S = 1)  = 0$ then the desired inequality is trivial. To handle the non-trival case, we note three facts from the proof of \Cref{lem:fuzzy}:
\begin{itemize}
    \item If $(t, u) \in m^{-1}([0, z])$ then $t \geq G^{-1}(z)$,
    \item If $(t, u) \in m^{-1}((z, 1])$ then $t \leq G^{-1}(z)$,
    \item The sets $m^{-1}([0, z])$ and $m^{-1}((z, 1])$ are disjoint.
\end{itemize}
Thus,
\begin{align*}
     \frac{1}{P_{\theta}(p \leq z | S = 1)} &= \frac{\int_{m^{-1}([0, 1])} g_{\theta}(t) s(t, u) \omega(dt, du) }{\int_{m^{-1}([0, z])} g_{\theta}(t) s(t, u) \omega(dt, du) }\\
                                            &= \frac{\int_{m^{-1}([0, z])} g_{\theta}(t) s(t, u) \omega(dt, du) + \int_{m^{-1}((z, 1])} g_{\theta}(t)  s(t, u)\omega(dt, du) }{\int_{m^{-1}([0, z])} g_{\theta}(t) s(t, u) \omega(dt, du) }\\
                                            &= 1 + \frac{\int_{m^{-1}((z, 1])} g_{\theta}(t) s(t, u) \omega(dt, du)}{\int_{m^{-1}([0, z])} g_{\theta}(t) s(t, u) \omega(dt, du)}\\
                                            &\geq 1 + \frac{g_{\theta}(G^{-1}(z))  \int_{m^{-1}((z, 1])}  s(t, u) \omega(dt, du)}{g_{\theta}(G^{-1}(z)) \int_{m^{-1}([0, z])} s(t, u) \omega(dt, du)} \\
                                            &= 1 + \frac{\int_{m^{-1}((z, 1])} s(t, u) \omega(dt, du)}{ \int_{m^{-1}([0, z])} s(t, u) \omega(dt, du)} \\
                                            &= \frac{1}{P_{\theta_0}(p \leq z | S = 1)}, 
\end{align*}
where to finish we have noted that $g_{\theta_0}(t) = 1$ almost eveywhere in the measure $\omega$. 
\end{proof}

\subsection{Exponential Families}
\label{sec:one_sided_exp_fam_appdx}

Suppose we observe data $X \in \R^m$ from an exponential family $P_{\theta}$ parameterized by $\theta \in \R^n$ i.e., under $P_{\theta}$ the data $X$ has density  
\begin{equation*}
    g_{\theta}(x) = \exp( \theta_1 T_1(x) + \dots + \theta_n T_n(x) - \psi(\theta) ) g(x) 
\end{equation*}
with respect to some carrier measure $\mu$. We consider the problem of testing $H_0: \theta_i \leq \theta_{0, i}$.

The UMPU test for $H_0: \theta_i \leq \theta_{0, i}$ is valid conditional on $T_{-i}(X)$. More specifically, Theorem 4.4.1 of \cite{Lehmann} tells us that any test of the form 

\begin{equation*}
    \label{eq:exp_fam_test}
    \phi(t_i, t_{-i}) = \begin{cases}
        1 &\text{if } t_i > C_0(t_{-i})  \\
        \gamma(t_{-i}) &\text{if } t_i = C_0(t_{-i})   \\
        0 & \text{otherwise }
    \end{cases}
\end{equation*}
where the functions $\gamma(\cdot)$ and $C_0(\cdot)$ satisfy 
\begin{equation*}
    E_{\theta_{0, i}}[\phi(T_i(X), t_{-i}) | T_{-i}(X) = t_{-i}] = \alpha
\end{equation*}
is UMPU for testing $H_0: \theta_i \leq \theta_{0, i}$. Lemma 2.7.2 of \cite{Lehmann} tells us that the conditional distribuition of $T_{i}(X)$ given $T_{-i}(X) = t_{-i}$ admits a density 

\begin{equation*}
    g_{\theta_i, t_{-i}}(t_i) = \exp(\theta_i t_i - \tilde{\psi}(\theta_i))  
\end{equation*}
with respect to some base measure $\mu_{t_{-i}}$. This density has an MLR in $t_i$ (to be specific, we are imagining observing $T_i(X)$ from its conditional distribution $T_{-i}(X)$, and the map $T(\cdot)$ from the previous sub-section is actually the identity). Hence, a concrete UMPU test is to just run our UMP test from the previous section using the conditional distribution given $T_{-i}(X) = t_{-i}$. In particular, our work from the previous section implies that it is UMPU to reject when the p-value

\begin{equation}
    \label{eq:ump_exp_fam}
    p = G^{+}(T_i(X)|T_{-i}(X)) + U_{aux}(G(T_i(X)|T_{-i}(X)) - G^{+}_i(T_i(X)|T_{-i}(X))),
\end{equation}
where $U_{aux}$ is an uniform random variable independent of the data and 
\begin{equation*}
    G(t_i | t_{-i}) = P_{\theta_0}(T_i(X) \geq t | T_{-i}(X) = t_{-i}) \qquad G^{+}(t_i | t_{-i}) = \lim_{u \downarrow t_i} G(u| t_{-i}),
\end{equation*}

is at most $\alpha$. Our work from the previous section also implies that this p-value is selectively dominant given $T_{-i}(X)$. 

\subsection{Montonicty of Selective MLR p-Values}
\label{sec:one_sided_monotone_appdx}

In this sub-section, we consider data $(X, Z)$ where the conditional distribution $X | Z= z \sim P_{\theta, z}$ is parameterized by $\theta \in \R$ and has an MLR in $T(x)$. Letting
\begin{equation*}
    G^{\theta_0}(t |z) = P_{\theta_0}(T(X) \geq t | Z=z) \qquad G^{\theta_0, +}(t|z) = \lim_{u \downarrow t} G^{\theta_0}(u|z)
\end{equation*}
we let 
\begin{equation*}
    p^{\theta_0} = G^{\theta_0, +}(T(X) |Z) + U_{aux}(G^{\theta_0}(T(X)|Z) - G^{\theta_0, +}(T(X))|Z)
\end{equation*}
be the UMP p-value for testing $H_{0}: \theta \leq \theta_0$ conditional on $Z=z$. Again, $U_{aux}$ is an uniform random variable independent of the data. Let 
\begin{equation*}
    m^{\theta_0}(t, u, z) = G^{\theta_0, +}(t|z) + u(G^{\theta_0}(t|z) - G^{\theta_0, +}(t|z))
\end{equation*}
be the map such that $m^{\theta_0}(T(X), U, Z) = p^{\theta_0}$. 

Considering a class of selection functions $s^{\theta_0}(x, z)$ under which $p^{\theta_0}$  and $U$ both have a positive probability of selection given $Z=z$, we want to show that the selective p-values 
\begin{equation*}
    p^{\theta_0}_{sel} = \frac{\int_{0}^{p^{\theta_0}} s^{\theta_0}(x) dx}{\int_{0}^{1} s^{\theta_0}(x) dx}.
\end{equation*}
are monotone non-decreasing in $\theta_0$. We will show that this is true the selection function $s^{\theta_0}(x, z)$ is independent of $\theta_0$ when written in terms of the data (i.e., the selection really depends on the data, not the null parameter being tested). That is, whenever there exists $\tilde{s}(\cdot, \cdot)$ such that 
\begin{equation*}
    s^{\theta_0}(x, z) = \tilde{s}(t, u, z) \text{ for all } t, u \text{ with } x = m^{\theta_0}(t, u, z), 
\end{equation*}

\begin{proposition}
    \label{prop:monotone_adjustment}
    Let $s_{\theta}(x)$ be a selection function such that for $\theta \leq \theta_0$, $s_{\theta_0}(x)= 0 \implies s_{\theta}(x) = 0$ and the ratio $s_{\theta_0}(x)/s_{\theta}(x)$ is non-decreasing in $x$. Then, if $p^{\theta}_{adj}$ is the adjustment from $F_{U | S_{\theta} = 1}(p^{\theta})$ then $p^{\theta}_{adj}$ is monotone non-decreasing increasing in $\theta$.  

\end{proposition}



\begin{proof}

        Fix a $t$ and $u$ and let $A_{r, s} = \{(t, u) : t > r \text{ or } t = r \text{ and } u \leq s \}$. Then it suffices to show that 
        \begin{equation*}
            \frac{ \int_{A_{t, u}}  g_{\theta}(t) s_{\theta}(t, u) \omega(dt, du)}{ \int g_{\theta}(t) s_{\theta}(t, u) \omega(dt, du)}
        \end{equation*}
        is monotone non-decreasing. Equivalently that  

        \begin{equation*}
            \frac{ \int_{A_{t, u}}  g_{\theta}(t) s_{\theta}(t, u) \omega(dt, du)}{ \int g_{\theta}(t) s_{\theta}(t, u) \omega(dt, du)} \leq \frac{ \int_{A_{r, s}} s_{\theta_0}(t, u) \omega(dt, du)}{ \int s_{\theta_0}(t, u) \omega(dt, du)}
        \end{equation*}

        If $\int_{A_{r, s}}  g_{\theta}(t) s_{\theta}(t, u) \omega(dt, du) = 0$ then the inequality is trivial. So it suffices to consider the other case and show that 

        \begin{equation*}
            \frac{ \int g_{\theta}(t) s_{\theta}(t, u) \omega(dt, du)} { \int_{A_{r, s}}  g_{\theta}(t) s_{\theta}(t, u) \omega(dt, du)} \geq \frac{ \int s_{\theta_0}(t, u) \omega(dt, du)}{ \int_{A_{r, s}} s_{\theta_0}(t, u) \omega(dt, du)}
        \end{equation*}

        \begin{equation*}
            1 + \frac{ \int_{A_{r, s}^c} g_{\theta}(t) s_{\theta}(t, u) \omega(dt, du)} { \int_{A_{r, s}}  g_{\theta}(t) s_{\theta}(t, u) \omega(dt, du)}\geq 1 + \frac{ \int_{A_{r, s}^c} s_{\theta_0}(t, u) \omega(dt, du)}{ \int_{A_{r, s}} s_{\theta_0}(t, u) \omega(dt, du)}
        \end{equation*}

        Definining $0/0 = 0$, our assumptions allow us to do the following:
        
        \begin{align*}
            \frac{ \int_{A_{r, s}^c} g_{\theta}(t) s_{\theta}(t, u) \omega(dt, du)} { \int_{A_{r, s}}  g_{\theta}(t) s_{\theta}(t, u) \omega(dt, du)} &=\frac{ \int_{A_{r, s}^c} g_{\theta}(t) s_{\theta_0}(t, u) \left( \frac{s_{\theta}(t, u) }{s_{\theta_0}(t, u)} \right) \omega(dt, du)} { \int_{A_{r, s}}  g_{\theta}(t) s_{\theta_0}(t, u) \left( \frac{s_{\theta}(t, u) }{s_{\theta_0}(t, u)} \right) \omega(dt, du)}\\
                &\geq \frac{g_{\theta}(G_{\theta}^{-1}(m(r, s))) \left( \frac{s_{\theta}(r, s) }{s_{\theta_0}(r, s)} \right)   \int_{A_{r, s}^c} s_{\theta_0}(t, u) \omega(dt, du)}{ g_{\theta}(G_{\theta}^{-1}(m(r, s))) \left( \frac{s_{\theta}(r, s) }{s_{\theta_0}(r, s)} \right)  \int_{A_{r, s}} s_{\theta_0}(t, u) \omega(dt, du)}\\
                &= \frac{  \int_{A_{r, s}^c} s_{\theta_0}(t, u) \omega(dt, du)}{   \int_{A_{r, s}} s_{\theta_0}(t, u) \omega(dt, du)}\\
        \end{align*}

\end{proof}

\subsection{additional}
\label{sec:sel_conf_region_appdx}

In parametric settings, we can  invert tests of data dependent null hypotheses to get selective confidence regions. These confidence regions cover a \underline{random} parameter. \Cref{exm:confidence_region} gives the beginnings of an illustrative example. 

\begin{example}[Confidence region for the winner] 
    \label{exm:confidence_region}
    Suppose the $X_i \sim P_{\theta_i}$ are $n$ independent samples from an MLR family $P_{\theta}$, and $p^{\theta_0}_j =  p^{\theta_0}(X_j)$ are the UMP p-values for testing $H_{0, j} : \theta_j \leq \theta_0$. Define the parameter vector $\Theta = (\theta_1, \dots, \theta_n)$ and let $W$ be the index of the smallest p-value, so that $X_W = \max_{i \in [n]} X_i$ is a winner. We can get a $1-\alpha$ confidence region for the winning parameter $\theta_W$ that is valid conditional on $W$ by inverting \Cref{exm:winner}'s test of the winning null $H_{0, W} : \theta_W \leq \theta_0$:
    \begin{align*}
        \label{eq:winner_cr}
        P_{\Theta}\left( \theta_W \in \left\{ \theta_0 : \frac{p^{\theta_0}_{(1)}}{p^{\theta_0}_{(2)}} > \alpha \right\} | W=j \right) &= P_{\Theta}\left( \theta_j \in \left\{ \theta_0 : \frac{p^{\theta_0}_{j}}{ \min_{i \neq j} p^{\theta_0}_i} > \alpha \right\} | W=j \right)\\
                                     &= P_{\Theta}\left(  \frac{p^{\theta_j}_{j}}{ \min_{i \neq j} p^{\theta_j}_i} > \alpha  | W=j \right)\\
                                    &= 1-\alpha,
    \end{align*} 
    where we have equality because $p^{\theta_j}_j$ has an exact uniform distribution given $p^{\theta_j}_{-j}$ under $P_{\Theta}$. 
\end{example}

Since the region in \Cref{exm:confidence_region} results from inverting a one sided test, we would hope that it is not just a confidence region, but a confidence lower bound. This is not obvious, however. While the p-values $p^{\theta_0}_j$ are known to be monotone in $\theta_0$, it is less clear whether the selective p-values $p^{\theta_0}_{j}/ \min_{i \neq j} p^{\theta_0}_i$ resulting from \Cref{thm:adjustment} are. Indeed confirming that selective confidence regions are actually invervals or lower bounds can be tricky in selective problems, and proving that they are is sometimes done on a case by case basis \citep{Benjamini, Lee} or not at all \citep{Sengupta}. 

It turns out that selective p-values resulting from UMP or UMPU one-sided testing are essentially always monotone in the null parameter $\theta_0$. Suppose we observe data $(X, Z)$ where the conditional distribution $X | Z= z \sim P_{\theta, z}$ is parameterized by $\theta \in \R$ and has an MLR in $T(x)$. This setting is general enough to encompass both when we have independent samples from an MLR family as in \Cref{exm:confidence_region} and also when we have data from an exponential family. Conditional on $Z = z$, the UMP p-value $p^{\theta_0}$ for testing $H_0: \theta \leq \theta_0$ is a deterministic function of $T(X)$ and an independent uniform $U_{aux}$ (see \Cref{sec:one_sided_appdx}):
\begin{equation}
    p^{\theta_0} = m^{\theta_0}(T(X), U_{aux}, z).
\end{equation}
We usually get the selective p-value $p^{\theta_0}_{sel}$ by applying \Cref{thm:adjustment}'s adjustment with the selection function $s^{\theta_0}(x, z)$. The resulting selective p-value $p^{\theta_0}_{sel}$ from \Cref{thm:adjustment} will be monotone non-decreasing in $\theta_0$ whenever $s^{\theta_0}(x, z)$ depends on $\theta_0$ only through $m^{\theta_0}$, i.e., if 
\begin{equation*}
   \text{there exists } \tilde{s} \text{ such that } s^{\theta_0}(x, z) = \tilde{s}(t, u, z) \text{ for all } t, u \text{ with } x = m^{\theta_0}(t, u, z).
\end{equation*}
Essentially, this condition requires that the selection event, once written in terms of the data, has no dependence on the specific parameter $\theta_0$ that we are testing. Admittedly, this is the one case where writing our problem in terms of p-values instead of the original data adds an additional complication. But, since this condition holds for essentially all selective problems, a practioner seldom has to worry about this complication.  

In \Cref{sec:winner_monotone}, we carefully show that the selective p-values from \Cref{exm:confidence_region} satisfy the above condition. This allows us to give a more refined version of our above example. 

\begin{example}[Confidence lower bound and interval for the winner] 
    \label{exm:confidence_interval}
    Recall the setting of \Cref{exm:confidence_region}. Then 
    \begin{equation}
        \label{eq:mlr_cond_lcb}
        \left\{ \theta_0 : \frac{p^{\theta_0}_{(1)}}{p^{\theta_0}_{(2)}} > \alpha \right\}
    \end{equation}
    and 
    \begin{equation}
        \label{eq:mlr_cond_ci}
        \left\{ \theta_0 : 1 - \frac{\alpha}{2}> \frac{p^{\theta_0}_{(1)}}{p^{\theta_0}_{(2)}} > \frac{\alpha}{2} \right\}
    \end{equation}
    are a CLB and CI that cover the winning parameter  $\theta_W$ with probability exactly $1-\alpha$ conditional on $W$. In the CI, we can replace the left bound with $1-\alpha_1$ and the right bound with $\alpha_2$ for any $\alpha_1, \alpha_2 $ such that $\alpha_1 + \alpha_2 = \alpha$. The choice $\alpha_1 = \alpha_2 = \alpha/2$ ensures we miscover because the CI lies above and below the parameter with equal probability. 
\end{example}

The CI and CLB from \Cref{exm:confidence_interval} immediately generalize the conditional CI and CLB for the winner that one gets by following \cite{Fithian2017} to any MLR family. These confidence regions are normally only presented in the Gaussian case, and we show explicitly in \Cref{sec:winner_monotone_appdx} that our CI and CLB match those in the existing literature in the Gaussian case. 

Something about p-values makes it easy to see that it only depends on the gap... 


\section{Selecting Multiple p-Values for Inference}
\label{sec:multiple_p_vals_appdx}

In this appendix, we generalize our selective dominance framework to allow us to select multiple p-values for inference. As we applied the results from \Cref{sec:dominance} in \Cref{exm:winner}, one can apply the results from this section to establish validity of all the methods we propose in \Cref{sec:multiple}. 

Suppose we have $n$ independent and selectively dominant p-values for the nulls $H_{0, i}$. We imagine conditioning on some collection of them, which, without loss of generality, we can take to be $Z = (p_{k+1}, \dots, p_n)$. Note that, due to independence, conditioning on $Z = z$ does not change the distribution of $p_j$. Thus, the $p_j$ remain p-values under the nulls $H_{0, j}$ that are independent. Now, for $1 \leq j \leq k$, we consider $k$ binary selection random variables $S_j \in \{0, 1\}$, where $S_j = 1$ when $p_j$ is selected. The relationship between $p_j$, $Z$, and $S_j$ is governed by the selection function 
\begin{equation*}
    s_j(x, z) = p(S_j = 1 | p_j = x, Z=z)
\end{equation*} 
Supposing that $U_j$ are independent uniform random variables that are also independent of, we can imagine selecting the $U_j$ using the same selection functions. That is, we can imagine that a binary selection variable $S'_j \in \{0, 1\}$ whose joint distribution with $U_j$ is governed by 
\begin{equation*}
    P(S'_j = 1| U_j = x, Z = z) = s_j(x, z)
\end{equation*}
So long as we consider selection functions $s_j(x, z)$ under which $p_j$ and $U_j$ both have positive probability of being selected given $Z=z$, then the machinery from \Cref{sec:dominance} tells us that 
\begin{equation*}
    p_{sel, j} = F_{U_j |Z, S_j' = 1}(p_j) = \frac{\int_0^{p_j} s_j(x, Z)dx }{\int_0^1 s_j(x, Z)dx}
\end{equation*}
is p-value (it stochastically dominates the uniform distribution under the null) conditional on $Z$ and selection $S_j = 1$. 

We will further assume that the selection happens independently, i.e., 
\begin{equation*}
    P(S_1 = 1, \dots, S_k = 1 \mid p_1, \dots, p_k, Z) = \prod_{j = 1}^k P(S_j = 1 \mid p_j, Z)
\end{equation*} 
By taking expectations conditional on $Z$ with respect to both sides, we find that the $S_j$ are independent given $Z$:
\begin{align*}
    P(S_1 = 1, \dots, S_k = 1 \mid Z) &= E[P(S_1 = 1, \dots, S_k = 1 | p_1, \dots, p_k, Z) \mid Z] \\
    &= E \left[\prod_{j = 1}^k P(S_j = 1 | p_j, Z) \;\middle|\; Z\right]\\
    &= \prod_{j=1}^k E[P(S_j = 1 | p_j, Z)]\\
    &= \prod_{j=1}^k P(S_j = 1 | Z)
\end{align*}
where we have used that the $p_j$ are conditionally independent given $Z$ to move the expectation inside the product. Finally, conditional on $Z$ and all the selections $S_j=1$, the adjusted p-values $p_{sel, j}$ are all independent of one another. 
\begin{equation*}
    P(p_{sel, 1} \in A_1, \dots, p_{sel, k} \in A_k \mid Z, S_1 = 1, \dots, S_k = 1) = \prod_{j=1}^k P(p_{sel, j} \in A_j \mid Z, S_j = 1)
\end{equation*}
This can be confirmed via Bayes rule. Consider a collection of sets $A_j$. Conditional on $Z=z$, the event $p_{sel, j} \in A_j$ can be written as $p_j \in B_{j, z}$ for some set $B_{j, z}$. Thus
\begin{align*}
    &P(p_{sel, 1} \in A_1, \dots, p_{sel, k} \in A_k \mid Z, S_1 = 1, \dots, S_k = 1) \\
    &= \frac{P(p_{sel, 1} \in A_1, \dots, p_{sel, k} \in A_k, | Z) P( S_1 = 1, \dots, S_k = 1 \mid Z, p_{sel, 1} \in A_1, \dots, p_{sel, k} \in A_k)  }{P(S_1=1, \dots, S_k = 1 | Z)}\\
    &= \frac{P(p_{1} \in B_{1, Z}, \dots, p_{k} \in B_{k, Z} | Z) P( S_1 = 1, \dots, S_k = 1 \mid Z, p_{1} \in B_{1, Z}, \dots, p_{k} \in B_{k, Z})  }{P(S_1=1, \dots, S_k = 1 | Z)}\\
    &= \prod_{j=1}^k \frac{ P(p_j \in B_{j, Z} |Z) P(S_j = 1 | Z, p_j \in B_{j, Z})}{P(S_j = 1 | Z) }\\
    &= \prod_{j=1}^k \frac{ P(p_{sel, j} \in A_j |Z) P(S_j = 1 | Z, p_{sel, j} \in A_j)}{P(S_j = 1 | Z) }\\
    &= \prod_{j = 1}^k P(p_{sel, j} \in A_j | Z, S_j=1),
\end{align*}
where we have used that the $p_j$ are independent conditional on $Z$.

\section{Additional Proofs}
\label{sec:proofs_appdx}

\subsection{Proof of \Cref{thm:adjustment}}
Recall we are considering a selection function such that the probability that $U$ is selected $P(S'=1 | Z= z) = \int_0^1 s(x, z) dx$ is positive. The CDF of $U$ given selection is continuous because it cannot have any point masses:
\begin{equation*}
    P(U = x | Z=z, S' = 1) = \frac{P(U = x, S' = 1| Z=z)}{P(S'=1 |Z=z)} \leq \frac{P(U = x|Z=z)}{P(S'=1|Z=z)} =  0. 
\end{equation*}
Therefore, defining 
\begin{equation*}
    F^{-1}_{U |Z=z, S'=1}(t)  = \inf \{x: F_{U |Z=z, S' = 1}(x) > t  \}
\end{equation*}
continuity implies that $F_{U|Z=z, S =1}(F^{-1}_{U |Z=z, S=1}(t)) = t$ and $F_{U | Z=z, S = 1}(x) \leq t \iff x \leq  F^{-1}_{U | Z=z, S = 1}(t)$. Then 
\begin{align*}
    P_{H_0}(F_{U | Z=z, S' = 1}(p) \leq  t |Z=z, S=1) &= P_{H_0}(p \leq F_{U |Z=z,  S' = 1}^{-1}(t) | Z=z, S=1) \\
    &\leq P(U \leq F_{U |Z=z, S' = 1}^{-1}(t) |Z=z, S'=1) \\
    &= P(F_{U|Z=z, S' = 1}(U) \leq t |Z=z, S'=1)
\end{align*}
where we have used that $p | Z=z, S=1 \succeq_{H_0} U |Z=z, S'=1$ to get the middle inequality. Finally 
\begin{align*}
    P(F_{U|Z=z, S' = 1}(U) \leq t |Z=z, S'=1) &= P(U \leq F_{U|S' = 1}^{-1}(t) | Z=z, S'=1) \\
    &= F_{U|Z=z, S'=1}(F_{U|Z=z, S' = 1}^{-1}(t)) = t
\end{align*}
so $F_{U|Z=z, S' = 1}(U) |Z=z, S' = 1 \sim \text{Unif}([0, 1])$, which finishes the proof. 

Further, if for some distribution under the null, $p$ has an exact uniform distribution given $Z=z$, then the distributions $U | Z = z, S= 1'$ and $p | Z= z , S=1$ are identical, so the fact that $F_{U|Z=z, S' = 1}(U) |Z=z, S' = 1 \sim \text{Unif}([0, 1])$ also implies that $F_{U|Z=z, S' = 1}(p) |Z=z, S = 1 \sim \text{Unif}([0, 1])$, and therefore \eqref{eq:adjusted_error_control} holds with equality in this case. 
 
\subsection{Proof of \Cref{thm:density}}
Let $f_z$ be the density of $p |Z=z$ under a distribution in the null $H_0$. We start by showing that, if $f_z$ is non-decreasing, then $p |Z =z, S =1$ dominates $U |Z=z, S'=1$ . Fixing a selection function $s(x, z)$, it suffices to show that for any $t \in [0, 1]$. 
    \begin{align*}
        &P(p \leq t | Z=z, S =1) \leq P(U \leq t |Z=z, S= 1)\\
        &\iff 
        \frac{\int_{0}^{t} s(x, z) f_z(x) dx }{\int_{0}^{1} s(x, z) f_z(x) dx } \leq \frac{\int_{0}^{t} s(x, z) dx}{\int_{0}^{1} s(x, z) dx } \\
    \end{align*}
If $P( p \leq t | Z=z, S = 1)$ is zero then this trivially holds. Otherwise $P( p \leq t | Z=z, S = 1) = \int_{0}^{t} s(x, z) f_z(x) dx  > 0$ and we see that, 
    \begin{align*}
        \frac{ \int_{0}^{1} s(x, z) f_z(x) dx }{\int_{0}^{t} s(x, z) f_z(x) dx } &= 1 + \frac{\int_{t}^{1} s(x, z) f_z(x) dx }{\int_{0}^{t} s(x, z) f_z(x) dx }\\
        &\geq 1 + \frac{f_z(t)\int_{t}^{1} s(x ,z) dx }{f_z(t)\int_{0}^{t} s(x, z)  dx }\\
        &= 1 + \frac{\int_{t}^{1} s(x, z) dx }{\int_{0}^{t} s(x, z)  dx }\\
        &= \frac{ \int_{0}^{1} s(x, z) dx }{\int_{0}^{t} s(x, z)  dx },
    \end{align*}
which is sufficient to imply the claim. 

Now assuming that $f_{z'}$ is continuous and not non-decreasing for some $z'$, we can show that $p$ is not selectively dominant. In general, it suffices for there to be two points $y_1 < y_2$ such that $f_{z'}$ is strictly larger in a neighborhood around $y_1$ than in a neighborhood around $y_2$, where these neighborhoods are disjoint. In particular for $\epsilon > 0$ let $N_{\epsilon}(y) = (y - \epsilon, y + \epsilon )$ be a ball around $y$. Then we need there to be $y_1$, $y_2$, $\epsilon > 0$, and some $\eta > 0$ such that, for all $w_1 \in N_{\epsilon}(y_1)$ and $w_2 \in N_{\epsilon}(y_2)$, $w_1  < w_2$ but $f_{z'}(w_2) + \eta < f_{z'}(w_1) $. If $f$ is continuous and not non-decreasing, then this must be true. First define $B_{high} = \inf \{f_{z'}(w_1): w_1 \in N_{\epsilon}(y_1)\}$ and  $B_{low} = \sup \{f_{z'}(w_2): w_2 \in N_{\epsilon}(y_2)\}$ so $B_{high}  > B_{low}$. Then consider the selection function 
\begin{equation*}
s(x, z')= \begin{cases}
1 &\text{if } x \in N_{\epsilon}(y_1) \cup N_{\epsilon}(y_2) \text{ and } z=z' \\
0 &\text{otherwise }
\end{cases}
\end{equation*}
and let $t$ be a value such that $t > w_1$ for all $w_1 \in N_{\epsilon}(y_1)$ and $t < w_2$ for all $w_2 \in  N_{\epsilon}(y_2)$. Trivially, 
\begin{equation*}
    P(U \leq t | Z = z', S' = 1) = \frac{1}{2}. 
\end{equation*}
But the fact that 
\begin{align*}
    \frac{1}{P(p \leq t | Z=z', S = 1)} &= \frac{ \int_{y_1 - \epsilon}^{y_1 + \epsilon} f_{z'}(x) dx + \int_{y_2 - \epsilon}^{y_2 + \epsilon} f_{z'}(x) dx  }{\int_{y_1 - \epsilon}^{y_1 + \epsilon} f_{z'}(x) dx}\\
    &= 1 + \frac{ \int_{y_2 - \epsilon}^{y_2 + \epsilon} f_{z'}(x) dx  }{\int_{y_1 - \epsilon}^{y_1 + \epsilon} f_{z'}(x) dx}\\
    &\leq 1 + \frac{ 2\epsilon B_{low}  }{2\epsilon B_{high} }\\
    & < 2
\end{align*}
implies that $P(p \leq t |Z=z',  S = 1) > \frac{1}{2} $, which means that $p$ is not selectively dominant. 

\subsection{Proof of \Cref{cor:cond_closed}}

It suffices to argue that closing our conditional global null testing procedure rejects $H_{0, (k)}$ if and only if $p_{(j)} \leq \alpha p_{(j + 1)} $ for every $1 \leq j \leq k$. We will define $p_{(n+1)} = 1$ and, for subsets $I \subseteq [p]$ of size one, we define the conditional procedure to reject the global null $H_{I, 0}$ when the lone p-value is at most $\alpha$. For subsets $I$ of size strictly more than one, the procedure rejects the global null $H_{I, 0}$ when the smallest p-value in $I$ is at most alpha times the second smallest p-value in $I$. \newline 

\noindent \textbf{Necessity: } For $1 \leq j \leq k$, let $I_{n-j + 1}$ be the size $n - j + 1$ subset that excludes the $j - 1$ smallest p-values for (when $j = 1$ then $I = [p]$). This subset includes the index of the $k$th smallest p-value, so we must reject $H_{0, I}$ to reject $H_{0, (k)}$. It rejects exactly when $p_{(j)} \leq \alpha p_{(j + 1)}$, so our conditions are neccesary. \newline 

\noindent \textbf{Sufficiency: } Consider a subset $I$ that contains the index of the $k$th smallest p-value. If it is size one, then we reject because
\begin{equation*}
    p_{(k)} \leq \alpha p_{(k+1)} \leq \alpha 
\end{equation*}
If it is of size at least two, suppose the smallest p-value in the set is then the $\ell$th smallest p-value for $\ell \leq k$ and the econd smallest p-value in $I$ is $m$th smallest p-value for some $m > \ell$. We will reject $H_{0, I}$ because 
\begin{equation*}
    p_{(\ell)} \leq \alpha p_{(\ell + 1)} \leq \alpha p_{(m)}.
\end{equation*}

\subsection{Proof of \Cref{cor:hyb}}
\label{sec:hyb_proof_appdx}

Suppose we have $n$ independent and selectively dominant p-values $p_i$ for the null hypotheses $H_{0, i}$. We restrict our attention to $j \in \mathcal{J}$ for which $p_j$ has positive probability of being the smallest. Suppose we use $p_j$ to test $H_{0, j}$ only when we observe that $p_j$ is strictly larger than $\beta_n$ but still the smallest of all the p-values. We can apply \Cref{sec:dominance}'s framework with $p=p_j$, $Z = p_{-j}$ and the selection function $s(x, z) = I_{\beta_n < p_j < \min_{i \neq j} p_i}$. It is straightforward to see that our adjusted p-value $p_{adj}$ is $(p_j - \beta_n)/(\min_{i \neq j} p_i - \beta_n)$, and \Cref{thm:adjustment} therefore tells us that 
\begin{equation*}
    P_{H_{0, j}}\left( \frac{p_j - \beta_n}{\min_{i \neq j} p_i - \beta_n} \leq \frac{\alpha -\beta}{1-\beta}  \;\middle|\; p_{-j}, S=1 \right) \leq \frac{\alpha - \beta}{1-\beta}.
\end{equation*}
Re-arranging things we get 
\begin{equation}
    \label{eq:hybrid_tool}
    P_{H_{0, j}}\left( p_j  \leq \frac{\alpha -\beta}{1-\beta}\min_{i \neq j} p_i  + \left( 1 - \frac{\alpha -\beta}{1-\beta}\right) \beta_n  \;\middle|\; p_{-j}, S=1  \right) \leq \frac{\alpha - \beta}{1-\beta}.
\end{equation}


Letting $W$ be the index of the smallest p-value, we can now prove the claim that rejecting $H_{0, W}$ when 
\begin{equation*}
    p_{(1)} \leq \frac{\alpha-\beta}{1-\alpha} p_{(2)} + \left(1 - \frac{\alpha-\beta}{1-\alpha} \right) \beta_n 
\end{equation*}
controls Type I error at level $\alpha$. Let $\widetilde{W}$ be the index of the smallest p-value if all the p-values are strictly larger than $\beta_n$. If some p-value is at most $\beta_n$, then force $\widetilde{W}=0$. Let $G_P$ be the event that a p-value corresponding to a true null is at most $\beta_n$ (note the event $G$ depends on the data generating process $P$). We know from Sidak's procedure (\Cref{thm:sidak_testing}) that $P(G_P) \leq \beta$. Three facts are immediate: 
\begin{equation*}
    G_P \subseteq \{\widetilde{W} = 0 \} \implies \{\widetilde{W} > 0 \} \subseteq G_P^c \implies P(\widetilde{W} > 0) \leq 1-\beta 
\end{equation*}

\begin{equation*}
    P(\text{falsely reject } H_{0, W}, G_P, \widetilde{W} = 0) \leq P(G_P) \leq \beta,
\end{equation*}

\begin{equation*}
    P(\text{falsely reject } H_{0, W}, G^c_P, \widetilde{W} = 0) = 0. 
\end{equation*}
If $H_{0, j}$ is true, the event $\widetilde{W} = j$ is the same event as selecting $p_j$ for inference in \eqref{eq:hybrid_tool}, so 
\begin{align*}
    P(\text{falsely reject } H_{0, W} | \widetilde{W} = j) &= P\left(p_{(1)} \leq \frac{\alpha-\beta}{1-\alpha} p_{(2)} + \left(1 - \frac{\alpha-\beta}{1-\alpha} \right) \beta_n | \widetilde{W} = j\right)\\
    &=  P\left(p_j \leq \frac{\alpha-\beta}{1-\alpha} \min_{i \neq j}p_i + \left(1 - \frac{\alpha-\beta}{1-\alpha} \right) \beta_n | \widetilde{W} = j\right)\\
    &\leq \frac{\alpha - \beta}{1-\alpha},
\end{align*}
and if $H_{0, j}$ is not true then trivially $P(\text{falsely reject } H_{0, W} | \widetilde{W} = j) = 0 \leq \frac{\alpha - \beta}{1-\alpha}$. Our result then follows from law of total probability:
\begin{align*}
    &P(\text{falsely reject} H_{0, W} )\\
    &= P(\text{falsely reject } H_{0, W}, \widetilde{W} = 0, G_P) + P(\text{falsely reject } H_{0, W}, \widetilde{W} = 0, G_P^c) \\
    &\qquad + \sum_{j \in \mathcal{J}}P(\text{falsely reject } H_{0, W} | \widetilde{W} = j ) P(\widetilde{W} = j)\\
    &\leq \beta + \frac{\alpha - \beta}{1-\beta} \sum_{j \in \mathcal{J}} P(\widetilde{W} = j)\\
    &= \beta + \frac{\alpha - \beta}{1-\beta} P(\widetilde{W} > 0)\\
    &\leq \alpha. 
\end{align*}

\subsection{Proof of \Cref{cor:hyb_closed}}

It suffices to argue that closing our hybrid global null testing procedure rejects $H_{0, (k)}$ if and only if 
\begin{equation*}
p_{(j)} \leq \frac{\alpha - \beta}{1-\beta} p_{(j + 1)} + \left(1 - \frac{\alpha - \beta}{1-\beta} \right)\beta_{n - j + 1} 
\end{equation*}
for every $1 \leq j \leq k$. We will define $p_{(n+1)} = \alpha$ so that the right-hand side of the above equals $\alpha$ when $j = n$. Correspondingly, for subsets $I \subseteq [p]$ of size one, we define the hybrid procedure to reject the global null $H_{I, 0}$ when the lone p-value is at most $\alpha$. For subsets $I$ of size strictly more than one, supposing that the smallest p-value in $I$ is the $\ell$th smallest p-value and the second smallest p-value in $I$ is the $m$th smallest p-value, the hybrid procedure rejects the global null $H_{I, 0}$ when
\begin{equation*}
    p_{(\ell)} \leq \frac{\alpha-\beta}{1-\beta} p_{(m)} + \left(1 - \frac{\alpha - \beta}{1-\beta} \right)\beta_{|I|}.
\end{equation*}

\noindent \textbf{Necessity: } For $1 \leq j \leq k$, let $I_{n-j + 1}$ be the size $n - j + 1$ subset that excludes the $j - 1$ smallest p-values (when $j = 1$ then $I = [p]$). This subset includes the index of the $k$th smallest p-value, so we must reject $H_{0, I}$ to reject $H_{0, (k)}$. It rejects exactly when
\begin{equation*}
    p_{(j)} \leq \frac{\alpha - \beta}{1-\beta} p_{(j + 1)} + \left(1 - \frac{\alpha - \beta}{1-\beta} \right)\beta_{n - j + 1}
\end{equation*}
so our conditions are neccesary. \newline 

\noindent \textbf{Sufficiency: } Consider a subset $I$ that contains the index of the $k$th smallest p-value. If it is size-one, then we reject because 
\begin{equation*}
    p_{(k)} \leq \frac{\alpha - \beta}{1-\beta}p_{(k + 1)} + \left(1 - \frac{\alpha - \beta}{1-\beta} \right) \beta_{n - k + 1} \leq \frac{\alpha - \beta}{1-\beta} + \left(1 - \frac{\alpha - \beta}{1-\beta} \right)\beta = \alpha. 
\end{equation*}
Now suppose that $I$ is size $n-j+1$.  Its smallest p-value is the $\ell$th smallest p-value for some $\ell \leq k$ and $\ell \leq j$, and its second smallest p-value is the $m$th smallest p-value for some $m > \ell$. We reject because 
\begin{align*}
    p_{(\ell)} &\leq \frac{\alpha-\beta}{1-\beta} p_{(\ell + 1)} + \left(1 - \frac{\alpha - \beta}{1-\beta} \right)\beta_{ n - \ell + 1}\\
    &\leq \frac{\alpha-\beta}{1-\beta} p_{(m)} + \left(1 - \frac{\alpha - \beta}{1-\beta} \right)\beta_{ n - j + 1} \\
    &= \frac{\alpha-\beta}{1-\beta} p_{(m)} + \left(1 - \frac{\alpha - \beta}{1-\beta} \right)\beta_{|I|}.
\end{align*}


\end{appendix}

\end{document}

